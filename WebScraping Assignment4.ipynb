{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 \n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\marjan\\anaconda\\lib\\site-packages (4.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in c:\\users\\marjan\\anaconda\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\marjan\\anaconda\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\marjan\\anaconda\\lib\\site-packages (from html5lib) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.html import read_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted 3 wikitables\n"
     ]
    }
   ],
   "source": [
    "page='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "wikitables=read_html(page, attrs={'class':'wikitable'})\n",
    "\n",
    "print('extracted {num} wikitables'.format(num=len(wikitables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Video name</th>\n",
       "      <th>Uploader / artist</th>\n",
       "      <th>Views (billions)</th>\n",
       "      <th>Upload date</th>\n",
       "      <th>Note</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[28]</td>\n",
       "      <td>Pinkfong Kids' Songs &amp; Stories</td>\n",
       "      <td>7.85</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>[B]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[30]</td>\n",
       "      <td>Luis Fonsi featuring Daddy Yankee</td>\n",
       "      <td>7.19</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>[C]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Shape of You\"[31]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>5.18</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>[D]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"See You Again\"[32]</td>\n",
       "      <td>Wiz Khalifa featuring Charlie Puth</td>\n",
       "      <td>4.94</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>[E]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[35]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>4.73</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>[F]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[36]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>4.41</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>[G]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Uptown Funk\"[37]</td>\n",
       "      <td>Mark Ronson featuring Bruno Mars</td>\n",
       "      <td>4.08</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>[H]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Gangnam Style\"[38]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>3.96</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>[I]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[40]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>3.69</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Bath Song\"[41]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>3.61</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[42]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>3.50</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Sorry\"[43]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>3.40</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Sugar\"[44]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>3.39</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Roar\"[45]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>3.27</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Counting Stars\"[46]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>3.19</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Thinking Out Loud\"[47]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>3.18</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Dame Tu Cosita\"[48]</td>\n",
       "      <td>El Chombo featuring Cutty Ranks</td>\n",
       "      <td>3.10</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Shake It Off\"[49]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>3.02</td>\n",
       "      <td>August 18, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Faded\"[50]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>2.98</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Lean On\"[51]</td>\n",
       "      <td>Major Lazer and DJ Snake featuring MØ</td>\n",
       "      <td>2.97</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Bailando\"[52]</td>\n",
       "      <td>Enrique Iglesias featuring Descemer Bueno and ...</td>\n",
       "      <td>2.97</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Dark Horse\"[53]</td>\n",
       "      <td>Katy Perry featuring Juicy J</td>\n",
       "      <td>2.96</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Girls Like You\"[54]</td>\n",
       "      <td>Maroon 5 featuring Cardi B</td>\n",
       "      <td>2.95</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Let Her Go\"[55]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>2.90</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Mi Gente\"[56]</td>\n",
       "      <td>J Balvin and Willy William</td>\n",
       "      <td>2.86</td>\n",
       "      <td>June 29, 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Hello\"[57]</td>\n",
       "      <td>Adele</td>\n",
       "      <td>2.78</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[58]</td>\n",
       "      <td>Shakira featuring Freshlyground</td>\n",
       "      <td>2.73</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Perfect\"[59]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>2.73</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Blank Space\"[60]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>2.71</td>\n",
       "      <td>November 10, 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Chantaje\"[61]</td>\n",
       "      <td>Shakira featuring Maluma</td>\n",
       "      <td>2.62</td>\n",
       "      <td>November 18, 2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "      <td>As of February 4, 2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       No.                                       Video name  \\\n",
       "0                       1.                           \"Baby Shark Dance\"[28]   \n",
       "1                       2.                                  \"Despacito\"[30]   \n",
       "2                       3.                               \"Shape of You\"[31]   \n",
       "3                       4.                              \"See You Again\"[32]   \n",
       "4                       5.                       \"Johny Johny Yes Papa\"[35]   \n",
       "5                       6.   \"Masha and the Bear – Recipe for Disaster\"[36]   \n",
       "6                       7.                                \"Uptown Funk\"[37]   \n",
       "7                       8.                              \"Gangnam Style\"[38]   \n",
       "8                       9.  \"Learning Colors – Colorful Eggs on a Farm\"[40]   \n",
       "9                      10.                                  \"Bath Song\"[41]   \n",
       "10                     11.                \"Phonics Song with Two Words\"[42]   \n",
       "11                     12.                                      \"Sorry\"[43]   \n",
       "12                     13.                                      \"Sugar\"[44]   \n",
       "13                     14.                                       \"Roar\"[45]   \n",
       "14                     15.                             \"Counting Stars\"[46]   \n",
       "15                     16.                          \"Thinking Out Loud\"[47]   \n",
       "16                     17.                             \"Dame Tu Cosita\"[48]   \n",
       "17                     18.                               \"Shake It Off\"[49]   \n",
       "18                     19.                                      \"Faded\"[50]   \n",
       "19                     20.                                    \"Lean On\"[51]   \n",
       "20                     21.                                   \"Bailando\"[52]   \n",
       "21                     22.                                 \"Dark Horse\"[53]   \n",
       "22                     23.                             \"Girls Like You\"[54]   \n",
       "23                     24.                                 \"Let Her Go\"[55]   \n",
       "24                     25.                                   \"Mi Gente\"[56]   \n",
       "25                     26.                                      \"Hello\"[57]   \n",
       "26                     27.           \"Waka Waka (This Time for Africa)\"[58]   \n",
       "27                     28.                                    \"Perfect\"[59]   \n",
       "28                     29.                                \"Blank Space\"[60]   \n",
       "29                     30.                                   \"Chantaje\"[61]   \n",
       "30  As of February 4, 2021                           As of February 4, 2021   \n",
       "\n",
       "                                    Uploader / artist        Views (billions)  \\\n",
       "0                      Pinkfong Kids' Songs & Stories                    7.85   \n",
       "1                   Luis Fonsi featuring Daddy Yankee                    7.19   \n",
       "2                                          Ed Sheeran                    5.18   \n",
       "3                  Wiz Khalifa featuring Charlie Puth                    4.94   \n",
       "4                                         LooLoo Kids                    4.73   \n",
       "5                                          Get Movies                    4.41   \n",
       "6                    Mark Ronson featuring Bruno Mars                    4.08   \n",
       "7                                                 Psy                    3.96   \n",
       "8                                         Miroshka TV                    3.69   \n",
       "9                          Cocomelon – Nursery Rhymes                    3.61   \n",
       "10                                          ChuChu TV                    3.50   \n",
       "11                                      Justin Bieber                    3.40   \n",
       "12                                           Maroon 5                    3.39   \n",
       "13                                         Katy Perry                    3.27   \n",
       "14                                        OneRepublic                    3.19   \n",
       "15                                         Ed Sheeran                    3.18   \n",
       "16                    El Chombo featuring Cutty Ranks                    3.10   \n",
       "17                                       Taylor Swift                    3.02   \n",
       "18                                        Alan Walker                    2.98   \n",
       "19              Major Lazer and DJ Snake featuring MØ                    2.97   \n",
       "20  Enrique Iglesias featuring Descemer Bueno and ...                    2.97   \n",
       "21                       Katy Perry featuring Juicy J                    2.96   \n",
       "22                         Maroon 5 featuring Cardi B                    2.95   \n",
       "23                                          Passenger                    2.90   \n",
       "24                         J Balvin and Willy William                    2.86   \n",
       "25                                              Adele                    2.78   \n",
       "26                    Shakira featuring Freshlyground                    2.73   \n",
       "27                                         Ed Sheeran                    2.73   \n",
       "28                                       Taylor Swift                    2.71   \n",
       "29                           Shakira featuring Maluma                    2.62   \n",
       "30                             As of February 4, 2021  As of February 4, 2021   \n",
       "\n",
       "               Upload date                    Note              Unnamed: 6  \n",
       "0            June 17, 2016                     [B]                     NaN  \n",
       "1         January 12, 2017                     [C]                     NaN  \n",
       "2         January 30, 2017                     [D]                     NaN  \n",
       "3            April 6, 2015                     [E]                     NaN  \n",
       "4          October 8, 2016                     [F]                     NaN  \n",
       "5         January 31, 2012                     [G]                     NaN  \n",
       "6        November 19, 2014                     [H]                     NaN  \n",
       "7            July 15, 2012                     [I]                     NaN  \n",
       "8        February 27, 2018                     NaN                     NaN  \n",
       "9              May 2, 2018                     NaN                     NaN  \n",
       "10           March 6, 2014                     NaN                     NaN  \n",
       "11        October 22, 2015                     NaN                     NaN  \n",
       "12        January 14, 2015                     NaN                     NaN  \n",
       "13       September 5, 2013                     NaN                     NaN  \n",
       "14            May 31, 2013                     NaN                     NaN  \n",
       "15         October 7, 2014                     NaN                     NaN  \n",
       "16           April 5, 2018                     NaN                     NaN  \n",
       "17         August 18, 2014                     NaN                     NaN  \n",
       "18        December 3, 2015                     NaN                     NaN  \n",
       "19          March 22, 2015                     NaN                     NaN  \n",
       "20          April 11, 2014                     NaN                     NaN  \n",
       "21       February 20, 2014                     NaN                     NaN  \n",
       "22            May 31, 2018                     NaN                     NaN  \n",
       "23           July 25, 2012                     NaN                     NaN  \n",
       "24           June 29, 2017                     NaN                     NaN  \n",
       "25        October 22, 2015                     NaN                     NaN  \n",
       "26            June 4, 2010                     NaN                     NaN  \n",
       "27        November 9, 2017                     NaN                     NaN  \n",
       "28       November 10, 2014                     NaN                     NaN  \n",
       "29       November 18, 2016                     NaN                     NaN  \n",
       "30  As of February 4, 2021  As of February 4, 2021  As of February 4, 2021  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikitables[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Scrape the details team India’s international fixtures from bcci.tv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.bcci.tv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix=driver.find_element_by_xpath(\"//ul[@class='footer-navigation']/li[2]/ul/li[1]/a\")\n",
    "try:\n",
    "    fix.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(fix.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Match_title=[]\n",
    "Series=[]\n",
    "Place=[] \n",
    "Date=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saturday\n",
      "13\n",
      "FEBRUARY\n",
      "09:30 IST\n",
      "Wednesday\n",
      "24\n",
      "FEBRUARY\n",
      "14:30 IST\n",
      "Thursday\n",
      "04\n",
      "MARCH\n",
      "09:30 IST\n",
      "Friday\n",
      "12\n",
      "MARCH\n",
      "19:00 IST\n",
      "Sunday\n",
      "14\n",
      "MARCH\n",
      "19:00 IST\n",
      "Tuesday\n",
      "16\n",
      "MARCH\n",
      "19:00 IST\n",
      "Thursday\n",
      "18\n",
      "MARCH\n",
      "19:00 IST\n",
      "Saturday\n",
      "20\n",
      "MARCH\n",
      "19:00 IST\n",
      "Tuesday\n",
      "23\n",
      "MARCH\n",
      "13:30 IST\n",
      "Friday\n",
      "26\n",
      "MARCH\n",
      "13:30 IST\n",
      "Sunday\n",
      "28\n",
      "MARCH\n",
      "13:30 IST\n",
      "Wednesday\n",
      "04\n",
      "AUGUST\n",
      "15:30 IST\n",
      "Thursday\n",
      "12\n",
      "AUGUST\n",
      "15:30 IST\n",
      "Wednesday\n",
      "25\n",
      "AUGUST\n",
      "15:30 IST\n",
      "Thursday\n",
      "02\n",
      "SEPTEMBER\n",
      "15:30 IST\n",
      "Friday\n",
      "10\n",
      "SEPTEMBER\n",
      "15:30 IST\n"
     ]
    }
   ],
   "source": [
    "dt=driver.find_elements_by_xpath(\"//div[@class='fixture__datetime desktop-only']\")\n",
    "for i in dt:\n",
    "    print(i.text)\n",
    "    Date.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST\n",
      "INDIA V ENGLAND 2021\n",
      "TEST\n",
      "INDIA V ENGLAND 2021\n",
      "TEST\n",
      "INDIA V ENGLAND 2021\n",
      "T20I\n",
      "INDIA V ENGLAND 2021\n",
      "T20I\n",
      "INDIA V ENGLAND 2021\n",
      "T20I\n",
      "INDIA V ENGLAND 2021\n",
      "T20I\n",
      "INDIA V ENGLAND 2021\n",
      "T20I\n",
      "INDIA V ENGLAND 2021\n",
      "ODI\n",
      "INDIA V ENGLAND 2021\n",
      "ODI\n",
      "INDIA V ENGLAND 2021\n",
      "ODI\n",
      "INDIA V ENGLAND 2021\n",
      "TEST\n",
      "ENGLAND V INDIA 2021\n",
      "TEST\n",
      "ENGLAND V INDIA 2021\n",
      "TEST\n",
      "ENGLAND V INDIA 2021\n",
      "TEST\n",
      "ENGLAND V INDIA 2021\n",
      "TEST\n",
      "ENGLAND V INDIA 2021\n"
     ]
    }
   ],
   "source": [
    "tt=driver.find_elements_by_xpath(\"//div[@class='fixture__format-strip']\")\n",
    "for i in tt:\n",
    "    print(i.text)\n",
    "    Match_title.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Test\n",
      "3rd Test\n",
      "4th Test\n",
      "1st T20I\n",
      "2nd T20I\n",
      "3rd T20I\n",
      "4th T20I\n",
      "5th T20I\n",
      "1st ODI\n",
      "2nd ODI\n",
      "3rd ODI\n",
      "1st Test\n",
      "2nd Test\n",
      "3rd Test\n",
      "4th Test\n",
      "5th Test\n"
     ]
    }
   ],
   "source": [
    "sr=driver.find_elements_by_xpath(\"//strong[@class='fixture__name fixture__name--with-margin']\")\n",
    "for i in sr:\n",
    "    print(i.text)\n",
    "    Series.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M. A. Chidambaram Stadium, Chennai\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Sardar Patel Stadium, Ahmedabad\n",
      "Maharashtra Cricket Association Stadium, Pune\n",
      "Maharashtra Cricket Association Stadium, Pune\n",
      "Maharashtra Cricket Association Stadium, Pune\n",
      "Trent Bridge, Nottingham\n",
      "Lord's, London\n",
      "Headingley, Leeds\n",
      "The Oval, London\n",
      "Old Trafford, Manchester\n"
     ]
    }
   ],
   "source": [
    "pl=driver.find_elements_by_xpath(\"//p[@class='fixture__additional-info']/span\")\n",
    "for i in pl:\n",
    "    print(i.text)\n",
    "    Place.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Title\":Match_title[0:10],\n",
    "                \"Date\":Date[0:10],\n",
    "                \"Venue\":Place[0:10],\n",
    "                \"Series\":Series[0:10]\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Saturday\\n13\\nFEBRUARY\\n09:30 IST</td>\n",
       "      <td>M. A. Chidambaram Stadium, Chennai</td>\n",
       "      <td>2nd Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Wednesday\\n24\\nFEBRUARY\\n14:30 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>3rd Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Thursday\\n04\\nMARCH\\n09:30 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>4th Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T20I\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Friday\\n12\\nMARCH\\n19:00 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>1st T20I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T20I\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Sunday\\n14\\nMARCH\\n19:00 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>2nd T20I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T20I\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Tuesday\\n16\\nMARCH\\n19:00 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>3rd T20I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T20I\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Thursday\\n18\\nMARCH\\n19:00 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>4th T20I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T20I\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Saturday\\n20\\nMARCH\\n19:00 IST</td>\n",
       "      <td>Sardar Patel Stadium, Ahmedabad</td>\n",
       "      <td>5th T20I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ODI\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Tuesday\\n23\\nMARCH\\n13:30 IST</td>\n",
       "      <td>Maharashtra Cricket Association Stadium, Pune</td>\n",
       "      <td>1st ODI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ODI\\nINDIA V ENGLAND 2021</td>\n",
       "      <td>Friday\\n26\\nMARCH\\n13:30 IST</td>\n",
       "      <td>Maharashtra Cricket Association Stadium, Pune</td>\n",
       "      <td>2nd ODI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title                                Date  \\\n",
       "0  TEST\\nINDIA V ENGLAND 2021   Saturday\\n13\\nFEBRUARY\\n09:30 IST   \n",
       "1  TEST\\nINDIA V ENGLAND 2021  Wednesday\\n24\\nFEBRUARY\\n14:30 IST   \n",
       "2  TEST\\nINDIA V ENGLAND 2021      Thursday\\n04\\nMARCH\\n09:30 IST   \n",
       "3  T20I\\nINDIA V ENGLAND 2021        Friday\\n12\\nMARCH\\n19:00 IST   \n",
       "4  T20I\\nINDIA V ENGLAND 2021        Sunday\\n14\\nMARCH\\n19:00 IST   \n",
       "5  T20I\\nINDIA V ENGLAND 2021       Tuesday\\n16\\nMARCH\\n19:00 IST   \n",
       "6  T20I\\nINDIA V ENGLAND 2021      Thursday\\n18\\nMARCH\\n19:00 IST   \n",
       "7  T20I\\nINDIA V ENGLAND 2021      Saturday\\n20\\nMARCH\\n19:00 IST   \n",
       "8   ODI\\nINDIA V ENGLAND 2021       Tuesday\\n23\\nMARCH\\n13:30 IST   \n",
       "9   ODI\\nINDIA V ENGLAND 2021        Friday\\n26\\nMARCH\\n13:30 IST   \n",
       "\n",
       "                                           Venue    Series  \n",
       "0             M. A. Chidambaram Stadium, Chennai  2nd Test  \n",
       "1                Sardar Patel Stadium, Ahmedabad  3rd Test  \n",
       "2                Sardar Patel Stadium, Ahmedabad  4th Test  \n",
       "3                Sardar Patel Stadium, Ahmedabad  1st T20I  \n",
       "4                Sardar Patel Stadium, Ahmedabad  2nd T20I  \n",
       "5                Sardar Patel Stadium, Ahmedabad  3rd T20I  \n",
       "6                Sardar Patel Stadium, Ahmedabad  4th T20I  \n",
       "7                Sardar Patel Stadium, Ahmedabad  5th T20I  \n",
       "8  Maharashtra Cricket Association Stadium, Pune   1st ODI  \n",
       "9  Maharashtra Cricket Association Stadium, Pune   2nd ODI  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException,NoSuchElementException,ElementNotInteractableException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.guru99.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel=driver.find_element_by_xpath(\"//i[@class='fa fa-code']\")\n",
    "try:\n",
    "    sel.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(sel.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exc=driver.find_element_by_xpath(\"/html/body/div[2]/section[3]/div/div[1]/main/div/div/div/div/div/div/div[2]/table[5]/tbody/tr[34]/td[1]/a/strong\")\n",
    "try:\n",
    "    exc.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(exc.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception name\n",
      "Description\n",
      "ElementNotVisibleException\n",
      "This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden.\n",
      "ElementNotSelectableException\n",
      "This Selenium exception occurs when an element is presented in the DOM, but you can be able to select. Therefore, it is not possible to interact.\n",
      "NoSuchElementException\n",
      "This Exception occurs if an element could not be found.\n",
      "NoSuchFrameException\n",
      "This Exception occurs if the frame target to be switched to does not exist.\n",
      "NoAlertPresentException\n",
      "This Exception occurs when you switch to no presented alert.\n",
      "NoSuchWindowException\n",
      "This Exception occurs if the window target to be switch does not exist.\n",
      "StaleElementReferenceException\n",
      "This Selenium exception occurs happens when the web element is detached from the current DOM.\n",
      "SessionNotFoundException\n",
      "The WebDriver is acting after you quit the browser.\n",
      "TimeoutException\n",
      "Thrown when there is not enough time for a command to be completed. For Example, the element searched wasn't found in the specified time.\n",
      "WebDriverException\n",
      "This Exception takes place when the WebDriver is acting right after you close the browser.\n",
      "ConnectionClosedException\n",
      "This type of Exception takes place when there is a disconnection in the driver.\n",
      "ElementClickInterceptedException\n",
      "The command may not be completed as the element receiving the events is concealing the element which was requested clicked.\n",
      "ElementNotInteractableException\n",
      "This Selenium exception is thrown when any element is presented in the DOM. However, it is impossible to interact with such an element.\n",
      "ErrorInResponseException\n",
      "This happens while interacting with the Firefox extension or the remote driver server.\n",
      "ErrorHandler.UnknownServerException\n",
      "Exception is used as a placeholder in case if the server returns an error without a stack trace.\n",
      "ImeActivationFailedException\n",
      "This expectation will occur when IME engine activation has failed.\n",
      "ImeNotAvailableException\n",
      "It takes place when IME support is unavailable.\n",
      "InsecureCertificateException\n",
      "Navigation made the user agent to hit a certificate warning. This can cause by an invalid or expired TLS certificate.\n",
      "InvalidArgumentException\n",
      "It occurs when an argument does not belong to the expected type.\n",
      "InvalidCookieDomainException\n",
      "This happens when you try to add a cookie under a different domain instead of current URL.\n",
      "InvalidCoordinatesException\n",
      "This type of Exception matches an interacting operation that is not valid.\n",
      "InvalidElementStateExceptio\n",
      "It occurs when command can't be finished when the element is invalid.\n",
      "InvalidSessionIdException\n",
      "This Exception took place when the given session ID is not included in the list of active sessions. It means the session does not exist or is inactive either.\n",
      "InvalidSwitchToTargetException\n",
      "This occurs when the frame or window target to be switched does not exist.\n",
      "JavascriptException\n",
      "This issue occurs while executing JavaScript given by the user.\n",
      "JsonException\n",
      "It occurs when you afford to get the session when the session is not created.\n",
      "NoSuchAttributeException\n",
      "This kind of Exception occurs when the attribute of an element could not be found.\n",
      "MoveTargetOutOfBoundsException\n",
      "It takes place if the target provided to the ActionChains move() methodology is not valid. For Example, out of the document.\n",
      "NoSuchContextException\n",
      "ContextAware does mobile device testing.\n",
      "NoSuchCookieException\n",
      "This Exception occurs when no cookie matching with the given pathname found for all the associated cookies of the currently browsing document.\n",
      "NotFoundException\n",
      "This Exception is a subclass of WebDriverException. This will occur when an element on the DOM does not exist.\n",
      "RemoteDriverServerException\n",
      "This Selenium exception is thrown when the server is not responding because of the problem that the capabilities described are not proper.\n",
      "ScreenshotException\n",
      "It is not possible to capture a screen.\n",
      "SessionNotCreatedException\n",
      "It happens when a new session could not be successfully created.\n",
      "UnableToSetCookieException\n",
      "This occurs if a driver is unable to set a cookie.\n",
      "UnexpectedTagNameException\n",
      "Happens if a support class did not get a web element as expected.\n",
      "UnhandledAlertException\n",
      "This expectation occurs when there is an alert, but WebDriver is not able to perform Alert operation.\n",
      "UnexpectedAlertPresentException\n",
      "It occurs when there is the appearance of an unexpected alert.\n",
      "UnknownMethodException\n",
      "This Exception happens when the requested command matches with a known URL but and not matching with a methodology for a specific URL.\n",
      "UnreachableBrowserException\n",
      "This Exception occurs only when the browser is not able to be opened or crashed because of some reason.\n",
      "UnsupportedCommandException\n",
      "This occurs when remote WebDriver does n't send valid commands as expected.\n"
     ]
    }
   ],
   "source": [
    "nm=driver.find_elements_by_xpath(\"//table[@class='table table-striped']/tbody/tr/td\")\n",
    "for i in nm:\n",
    "    print(i.text)\n",
    "    exceptions.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"http://statisticstimes.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec=driver.find_element_by_xpath(\"/html/body/div[2]/div[1]/div[2]/div[2]/div/a[3]\")\n",
    "try:\n",
    "    ec.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(ec.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "try:\n",
    "    gd.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(gd.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rank=[]\n",
    "State=[]\n",
    "GSDP18=[]\n",
    "GSDP19=[]\n",
    "Share17=[]\n",
    "GDPbillion=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "rn=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[1]\")\n",
    "for i in rn:\n",
    "    print(i.text)\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maharashtra\n",
      "Uttar Pradesh\n",
      "Tamil Nadu\n",
      "Karnataka\n",
      "Gujarat\n",
      "West Bengal\n",
      "Rajasthan\n",
      "Andhra Pradesh\n",
      "Telangana\n",
      "Madhya Pradesh\n",
      "Kerala\n",
      "Delhi\n",
      "Haryana\n",
      "Bihar\n",
      "Punjab\n",
      "Odisha\n",
      "Assam\n",
      "Chhattisgarh\n",
      "Jharkhand\n",
      "Uttarakhand\n",
      "Jammu & Kashmir\n",
      "Himachal Pradesh\n",
      "Goa\n",
      "Tripura\n",
      "Chandigarh\n",
      "Puducherry\n",
      "Meghalaya\n",
      "Sikkim\n",
      "Manipur\n",
      "Nagaland\n",
      "Arunachal Pradesh\n",
      "Mizoram\n",
      "Andaman & Nicobar Islands\n",
      "Maharashtra\n",
      "Uttar Pradesh\n",
      "Tamil Nadu\n",
      "Karnataka\n",
      "Gujarat\n",
      "West Bengal\n",
      "Rajasthan\n",
      "Telangana\n",
      "Andhra Pradesh\n",
      "Madhya Pradesh\n",
      "Kerala\n",
      "Delhi\n",
      "Haryana\n",
      "Bihar\n",
      "Punjab\n",
      "Odisha\n",
      "Assam\n",
      "Jharkhand\n",
      "Chhattisgarh\n",
      "Uttarakhand\n",
      "Himachal Pradesh\n",
      "Jammu & Kashmir\n",
      "Goa\n",
      "Tripura\n",
      "Chandigarh\n",
      "Puducherry\n",
      "Meghalaya\n",
      "Manipur\n",
      "Sikkim\n",
      "Nagaland\n",
      "Arunachal Pradesh\n",
      "Mizoram\n",
      "Andaman & Nicobar Islands\n"
     ]
    }
   ],
   "source": [
    "st=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[2]\")\n",
    "for i in st:\n",
    "    print(i.text)\n",
    "    State.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,632,792\n",
      "1,668,229\n",
      "1,630,208\n",
      "1,544,399\n",
      "1,502,899\n",
      "1,089,898\n",
      "942,586\n",
      "862,957\n",
      "861,031\n",
      "809,592\n",
      "781,653\n",
      "774,870\n",
      "734,163\n",
      "530,363\n",
      "526,376\n",
      "492,229\n",
      "315,881\n",
      "304,063\n",
      "297,204\n",
      "245,895\n",
      "155,956\n",
      "153,845\n",
      "73,170\n",
      "49,845\n",
      "42,114\n",
      "36,656\n",
      "33,481\n",
      "28,723\n",
      "27,869\n",
      "27,283\n",
      "24,603\n",
      "19,520\n",
      "-\n",
      "2,332,992\n",
      "1,491,311\n",
      "1,465,361\n",
      "1,409,126\n",
      "1,322,936\n",
      "995,502\n",
      "845,247\n",
      "782,370\n",
      "776,140\n",
      "737,156\n",
      "707,542\n",
      "704,529\n",
      "666,075\n",
      "486,776\n",
      "472,506\n",
      "432,455\n",
      "282,782\n",
      "271,990\n",
      "266,537\n",
      "221,871\n",
      "133,303\n",
      "129,877\n",
      "66,060\n",
      "44,835\n",
      "37,571\n",
      "33,598\n",
      "29,544\n",
      "25,322\n",
      "25,141\n",
      "24,534\n",
      "22,488\n",
      "17,506\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "gs18=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[4]\")\n",
    "for i in gs18:\n",
    "    print(i.text)\n",
    "    GSDP18.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "1,794,508\n",
      "1,845,853\n",
      "1,699,115\n",
      "-\n",
      "1,253,832\n",
      "1,020,989\n",
      "972,782\n",
      "969,604\n",
      "906,672\n",
      "-\n",
      "856,112\n",
      "831,610\n",
      "611,804\n",
      "574,760\n",
      "531,374\n",
      "-\n",
      "329,180\n",
      "328,598\n",
      "-\n",
      "-\n",
      "165,472\n",
      "-\n",
      "55,358\n",
      "-\n",
      "40,802\n",
      "36,572\n",
      "32,496\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "1,603,083\n",
      "1,659,210\n",
      "1,550,297\n",
      "-\n",
      "1,150,711\n",
      "916,014\n",
      "881,873\n",
      "875,429\n",
      "827,019\n",
      "-\n",
      "779,647\n",
      "755,790\n",
      "562,710\n",
      "517,521\n",
      "467,554\n",
      "-\n",
      "301,242\n",
      "288,041\n",
      "-\n",
      "143,063\n",
      "-\n",
      "-\n",
      "49,601\n",
      "-\n",
      "37,134\n",
      "32,833\n",
      "-\n",
      "28,391\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "gs19=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[3]\")\n",
    "for i in gs19:\n",
    "    print(i.text)\n",
    "    GSDP19.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.88%\n",
      "8.79%\n",
      "8.59%\n",
      "8.14%\n",
      "7.92%\n",
      "5.75%\n",
      "4.97%\n",
      "4.55%\n",
      "4.54%\n",
      "4.27%\n",
      "4.12%\n",
      "4.08%\n",
      "3.87%\n",
      "2.80%\n",
      "2.77%\n",
      "2.59%\n",
      "1.67%\n",
      "1.60%\n",
      "1.57%\n",
      "1.30%\n",
      "0.82%\n",
      "0.81%\n",
      "0.39%\n",
      "0.26%\n",
      "0.22%\n",
      "0.19%\n",
      "0.18%\n",
      "0.15%\n",
      "0.15%\n",
      "0.14%\n",
      "0.13%\n",
      "0.10%\n",
      "-\n",
      "13.90%\n",
      "8.88%\n",
      "8.73%\n",
      "8.39%\n",
      "7.88%\n",
      "5.93%\n",
      "5.03%\n",
      "4.66%\n",
      "4.62%\n",
      "4.39%\n",
      "4.21%\n",
      "4.20%\n",
      "3.97%\n",
      "2.90%\n",
      "2.81%\n",
      "2.58%\n",
      "1.68%\n",
      "1.62%\n",
      "1.59%\n",
      "1.32%\n",
      "0.79%\n",
      "0.77%\n",
      "0.39%\n",
      "0.27%\n",
      "0.22%\n",
      "0.20%\n",
      "0.18%\n",
      "0.15%\n",
      "0.15%\n",
      "0.15%\n",
      "0.13%\n",
      "0.10%\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "sh17=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[5]\")\n",
    "for i in sh17:\n",
    "    print(i.text)\n",
    "    Share17.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398.145\n",
      "252.278\n",
      "246.529\n",
      "233.552\n",
      "227.276\n",
      "164.820\n",
      "142.543\n",
      "130.501\n",
      "130.210\n",
      "122.431\n",
      "118.206\n",
      "117.180\n",
      "111.024\n",
      "80.204\n",
      "79.601\n",
      "74.437\n",
      "47.769\n",
      "45.982\n",
      "44.945\n",
      "37.186\n",
      "23.584\n",
      "23.265\n",
      "11.065\n",
      "7.538\n",
      "6.369\n",
      "5.543\n",
      "5.063\n",
      "4.344\n",
      "4.214\n",
      "4.126\n",
      "3.721\n",
      "2.952\n",
      "-\n",
      "-\n",
      "1,039,180\n",
      "1,167,776\n",
      "1,085,599\n",
      "-\n",
      "713,376\n",
      "630,693\n",
      "594,806\n",
      "595,605\n",
      "496,798\n",
      "-\n",
      "568,265\n",
      "514,983\n",
      "377,276\n",
      "374,015\n",
      "351,661\n",
      "-\n",
      "218,232\n",
      "210,837\n",
      "-\n",
      "107,171\n",
      "-\n",
      "-\n",
      "35,821\n",
      "-\n",
      "23,591\n",
      "23,564\n",
      "-\n",
      "17,060\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "gdp=driver.find_elements_by_xpath(\"//div[@class='dataTables_wrapper']/table/tbody/tr/td[6]\")\n",
    "for i in gdp:\n",
    "    print(i.text)\n",
    "    GDPbillion.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Rank\":Rank[0:33],\n",
    "                \"State\":State[0:33],\n",
    "                \"GSDP(18-19)\":GSDP18[0:33],\n",
    "                \"GSDP(17-18)\":GSDP19[0:33],\n",
    "                 \"Share(2017)\":Share17[0:33],\n",
    "                 \"GDP($ billion)\":GDPbillion[0:33]\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP(18-19)</th>\n",
       "      <th>GSDP(17-18)</th>\n",
       "      <th>Share(2017)</th>\n",
       "      <th>GDP($ billion)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.88%</td>\n",
       "      <td>398.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,668,229</td>\n",
       "      <td>1,794,508</td>\n",
       "      <td>8.79%</td>\n",
       "      <td>252.278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.59%</td>\n",
       "      <td>246.529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,544,399</td>\n",
       "      <td>1,699,115</td>\n",
       "      <td>8.14%</td>\n",
       "      <td>233.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.92%</td>\n",
       "      <td>227.276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.75%</td>\n",
       "      <td>164.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.97%</td>\n",
       "      <td>142.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.55%</td>\n",
       "      <td>130.501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.54%</td>\n",
       "      <td>130.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.27%</td>\n",
       "      <td>122.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.12%</td>\n",
       "      <td>118.206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.08%</td>\n",
       "      <td>117.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.87%</td>\n",
       "      <td>111.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.80%</td>\n",
       "      <td>80.204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.77%</td>\n",
       "      <td>79.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>492,229</td>\n",
       "      <td>531,374</td>\n",
       "      <td>2.59%</td>\n",
       "      <td>74.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.60%</td>\n",
       "      <td>45.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>44.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.82%</td>\n",
       "      <td>23.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>-</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,358</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>36,656</td>\n",
       "      <td>40,802</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>5.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,869</td>\n",
       "      <td>-</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>19,520</td>\n",
       "      <td>-</td>\n",
       "      <td>0.10%</td>\n",
       "      <td>2.952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP(18-19) GSDP(17-18) Share(2017)  \\\n",
       "0     1                Maharashtra   2,632,792           -      13.88%   \n",
       "1     2              Uttar Pradesh   1,668,229   1,794,508       8.79%   \n",
       "2     3                 Tamil Nadu   1,630,208   1,845,853       8.59%   \n",
       "3     4                  Karnataka   1,544,399   1,699,115       8.14%   \n",
       "4     5                    Gujarat   1,502,899           -       7.92%   \n",
       "5     6                West Bengal   1,089,898   1,253,832       5.75%   \n",
       "6     7                  Rajasthan     942,586   1,020,989       4.97%   \n",
       "7     8             Andhra Pradesh     862,957     972,782       4.55%   \n",
       "8     9                  Telangana     861,031     969,604       4.54%   \n",
       "9    10             Madhya Pradesh     809,592     906,672       4.27%   \n",
       "10   11                     Kerala     781,653           -       4.12%   \n",
       "11   12                      Delhi     774,870     856,112       4.08%   \n",
       "12   13                    Haryana     734,163     831,610       3.87%   \n",
       "13   14                      Bihar     530,363     611,804       2.80%   \n",
       "14   15                     Punjab     526,376     574,760       2.77%   \n",
       "15   16                     Odisha     492,229     531,374       2.59%   \n",
       "16   17                      Assam     315,881           -       1.67%   \n",
       "17   18               Chhattisgarh     304,063     329,180       1.60%   \n",
       "18   19                  Jharkhand     297,204     328,598       1.57%   \n",
       "19   20                Uttarakhand     245,895           -       1.30%   \n",
       "20   21            Jammu & Kashmir     155,956           -       0.82%   \n",
       "21   22           Himachal Pradesh     153,845     165,472       0.81%   \n",
       "22   23                        Goa      73,170           -       0.39%   \n",
       "23   24                    Tripura      49,845      55,358       0.26%   \n",
       "24   25                 Chandigarh      42,114           -       0.22%   \n",
       "25   26                 Puducherry      36,656      40,802       0.19%   \n",
       "26   27                  Meghalaya      33,481      36,572       0.18%   \n",
       "27   28                     Sikkim      28,723      32,496       0.15%   \n",
       "28   29                    Manipur      27,869           -       0.15%   \n",
       "29   30                   Nagaland      27,283           -       0.14%   \n",
       "30   31          Arunachal Pradesh      24,603           -       0.13%   \n",
       "31   32                    Mizoram      19,520           -       0.10%   \n",
       "32   33  Andaman & Nicobar Islands           -           -           -   \n",
       "\n",
       "   GDP($ billion)  \n",
       "0         398.145  \n",
       "1         252.278  \n",
       "2         246.529  \n",
       "3         233.552  \n",
       "4         227.276  \n",
       "5         164.820  \n",
       "6         142.543  \n",
       "7         130.501  \n",
       "8         130.210  \n",
       "9         122.431  \n",
       "10        118.206  \n",
       "11        117.180  \n",
       "12        111.024  \n",
       "13         80.204  \n",
       "14         79.601  \n",
       "15         74.437  \n",
       "16         47.769  \n",
       "17         45.982  \n",
       "18         44.945  \n",
       "19         37.186  \n",
       "20         23.584  \n",
       "21         23.265  \n",
       "22         11.065  \n",
       "23          7.538  \n",
       "24          6.369  \n",
       "25          5.543  \n",
       "26          5.063  \n",
       "27          4.344  \n",
       "28          4.214  \n",
       "29          4.126  \n",
       "30          3.721  \n",
       "31          2.952  \n",
       "32              -  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://github.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex=driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "try:\n",
    "    ex.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(ex.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Repository_title=[]\n",
    "Repository_description=[]\n",
    "Contributors_count=[]\n",
    "Language_used=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JonnyBurger / remotion-trailer\n",
      "iam-abbas / Reddit-Stock-Trends\n",
      "hashicorp / vault\n",
      "conwnet / github1s\n",
      "Azure / azure-quickstart-templates\n",
      "Kaiyiwing / qwerty-learner\n",
      "JonnyBurger / remotion\n",
      "nemanjarogic / DesignPatternsLibrary\n",
      "tgbot-collection / YYeTsBot\n",
      "aws / amazon-sagemaker-examples\n",
      "jomjol / AI-on-the-edge-device\n",
      "goldbergyoni / nodebestpractices\n",
      "streamich / react-use\n",
      "ryanmcdermott / clean-code-javascript\n",
      "charliegerard / gaze-detection\n",
      "mozilla-mobile / firefox-ios\n",
      "dkhamsing / open-source-ios-apps\n",
      "justjavac / 1s\n",
      "nandorojo / moti\n",
      "sveltejs / svelte\n",
      "googlesamples / mlkit\n",
      "ansible / awx\n",
      "reed-hong / awesome-libra\n",
      "abuanwar072 / Flutter-responsive-email-ui---Mobile-Tablet-and-Web\n",
      "Lolliedieb / lolMiner-releases\n"
     ]
    }
   ],
   "source": [
    "tt=driver.find_elements_by_xpath(\"//h1[@class='h3 lh-condensed']/a\")\n",
    "for i in tt:\n",
    "    print(i.text)\n",
    "    Repository_title.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A video written in React\n",
      "Fetch currently trending stocks on Reddit\n",
      "A tool for secrets management, encryption as a service, and privileged access management\n",
      "One second to read GitHub code with VS Code.\n",
      "Azure Quickstart Templates\n",
      "为键盘工作者设计的单词记忆与英语肌肉记忆锻炼软件 / Words learning and English muscle memory training software designed for keyboard workers\n",
      "🎥 Create videos programmatically in React\n",
      "A comprehensive design patterns library implemented in C#, which covers various design patterns from the most commonly used ones to the lesser-known ones. Get familiar with and learn design patterns through moderately realistic examples.\n",
      "🎬 人人影视bot，完全对接人人影视全部无删减资源\n",
      "Example notebooks that show how to apply machine learning, deep learning and reinforcement learning in Amazon SageMaker\n",
      "✅ The Node.js best practices list (February 2021)\n",
      "React Hooks — 👍\n",
      "🛁 Clean Code concepts adapted for JavaScript\n",
      "👀 Use machine learning in JavaScript to detect eye movements and build gaze-controlled experiences.\n",
      "Firefox for iOS\n",
      "📱 Collaborative List of Open-Source iOS Apps\n",
      "天若有情天亦老，我为网站加一秒\n",
      "🐼 The universal React Native animation library, powered by Reanimated 2.\n",
      "Cybernetically enhanced web apps\n",
      "A collection of sample apps to demonstrate how to use Google's ML Kit APIs on Android and iOS\n",
      "AWX Project\n",
      "A Curated List of Awesome Facebook Libra Resources\n",
      "We redesign the outlook app also make it responsive so that you can run it everywhere on your phone, tab, or web.\n"
     ]
    }
   ],
   "source": [
    "dd=driver.find_elements_by_xpath(\"//p[@class='col-9 text-gray my-1 pr-4']\")\n",
    "for i in dd:\n",
    "    print(i.text)\n",
    "    Repository_description.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "134\n",
      "2,787\n",
      "152\n",
      "12,210\n",
      "85\n",
      "106\n",
      "16\n",
      "319\n",
      "3,739\n",
      "43\n",
      "5,816\n",
      "1,364\n",
      "5,544\n",
      "12\n",
      "2,336\n",
      "4,247\n",
      "7\n",
      "14\n",
      "2,045\n",
      "710\n",
      "2,323\n",
      "155\n",
      "99\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "cc=driver.find_elements_by_xpath(\"//div[@class='f6 text-gray mt-2']/a[2]\")\n",
    "for i in cc:\n",
    "    print(i.text)\n",
    "    Contributors_count.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TypeScript\n",
      "Python\n",
      "Go\n",
      "TypeScript\n",
      "PowerShell\n",
      "TypeScript\n",
      "TypeScript\n",
      "C#\n",
      "Python\n",
      "Jupyter Notebook\n",
      "C++\n",
      "JavaScript\n",
      "TypeScript\n",
      "JavaScript\n",
      "JavaScript\n",
      "Swift\n",
      "Swift\n",
      "TypeScript\n",
      "TypeScript\n",
      "Java\n",
      "Python\n",
      "Dart\n"
     ]
    }
   ],
   "source": [
    "ll=driver.find_elements_by_xpath(\"//span[@class='d-inline-block ml-0 mr-3']\")\n",
    "for i in ll:\n",
    "    print(i.text)\n",
    "    Language_used.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Repository title\":Repository_title[0:15],\n",
    "                \"Repository description\":Repository_description[0:15],\n",
    "                \"Contributors count\":Contributors_count[0:15],\n",
    "                \"Language used\":Language_used[0:15]\n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository title</th>\n",
       "      <th>Repository description</th>\n",
       "      <th>Contributors count</th>\n",
       "      <th>Language used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JonnyBurger / remotion-trailer</td>\n",
       "      <td>A video written in React</td>\n",
       "      <td>36</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>iam-abbas / Reddit-Stock-Trends</td>\n",
       "      <td>Fetch currently trending stocks on Reddit</td>\n",
       "      <td>134</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hashicorp / vault</td>\n",
       "      <td>A tool for secrets management, encryption as a...</td>\n",
       "      <td>2,787</td>\n",
       "      <td>Go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conwnet / github1s</td>\n",
       "      <td>One second to read GitHub code with VS Code.</td>\n",
       "      <td>152</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Azure / azure-quickstart-templates</td>\n",
       "      <td>Azure Quickstart Templates</td>\n",
       "      <td>12,210</td>\n",
       "      <td>PowerShell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kaiyiwing / qwerty-learner</td>\n",
       "      <td>为键盘工作者设计的单词记忆与英语肌肉记忆锻炼软件 / Words learning and ...</td>\n",
       "      <td>85</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JonnyBurger / remotion</td>\n",
       "      <td>🎥 Create videos programmatically in React</td>\n",
       "      <td>106</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nemanjarogic / DesignPatternsLibrary</td>\n",
       "      <td>A comprehensive design patterns library implem...</td>\n",
       "      <td>16</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tgbot-collection / YYeTsBot</td>\n",
       "      <td>🎬 人人影视bot，完全对接人人影视全部无删减资源</td>\n",
       "      <td>319</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aws / amazon-sagemaker-examples</td>\n",
       "      <td>Example notebooks that show how to apply machi...</td>\n",
       "      <td>3,739</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jomjol / AI-on-the-edge-device</td>\n",
       "      <td>✅ The Node.js best practices list (February 2021)</td>\n",
       "      <td>43</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>goldbergyoni / nodebestpractices</td>\n",
       "      <td>React Hooks — 👍</td>\n",
       "      <td>5,816</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>streamich / react-use</td>\n",
       "      <td>🛁 Clean Code concepts adapted for JavaScript</td>\n",
       "      <td>1,364</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ryanmcdermott / clean-code-javascript</td>\n",
       "      <td>👀 Use machine learning in JavaScript to detect...</td>\n",
       "      <td>5,544</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>charliegerard / gaze-detection</td>\n",
       "      <td>Firefox for iOS</td>\n",
       "      <td>12</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Repository title  \\\n",
       "0          JonnyBurger / remotion-trailer   \n",
       "1         iam-abbas / Reddit-Stock-Trends   \n",
       "2                       hashicorp / vault   \n",
       "3                      conwnet / github1s   \n",
       "4      Azure / azure-quickstart-templates   \n",
       "5              Kaiyiwing / qwerty-learner   \n",
       "6                  JonnyBurger / remotion   \n",
       "7    nemanjarogic / DesignPatternsLibrary   \n",
       "8             tgbot-collection / YYeTsBot   \n",
       "9         aws / amazon-sagemaker-examples   \n",
       "10         jomjol / AI-on-the-edge-device   \n",
       "11       goldbergyoni / nodebestpractices   \n",
       "12                  streamich / react-use   \n",
       "13  ryanmcdermott / clean-code-javascript   \n",
       "14         charliegerard / gaze-detection   \n",
       "\n",
       "                               Repository description Contributors count  \\\n",
       "0                            A video written in React                 36   \n",
       "1           Fetch currently trending stocks on Reddit                134   \n",
       "2   A tool for secrets management, encryption as a...              2,787   \n",
       "3        One second to read GitHub code with VS Code.                152   \n",
       "4                          Azure Quickstart Templates             12,210   \n",
       "5   为键盘工作者设计的单词记忆与英语肌肉记忆锻炼软件 / Words learning and ...                 85   \n",
       "6           🎥 Create videos programmatically in React                106   \n",
       "7   A comprehensive design patterns library implem...                 16   \n",
       "8                           🎬 人人影视bot，完全对接人人影视全部无删减资源                319   \n",
       "9   Example notebooks that show how to apply machi...              3,739   \n",
       "10  ✅ The Node.js best practices list (February 2021)                 43   \n",
       "11                                    React Hooks — 👍              5,816   \n",
       "12       🛁 Clean Code concepts adapted for JavaScript              1,364   \n",
       "13  👀 Use machine learning in JavaScript to detect...              5,544   \n",
       "14                                    Firefox for iOS                 12   \n",
       "\n",
       "       Language used  \n",
       "0         TypeScript  \n",
       "1             Python  \n",
       "2                 Go  \n",
       "3         TypeScript  \n",
       "4         PowerShell  \n",
       "5         TypeScript  \n",
       "6         TypeScript  \n",
       "7                 C#  \n",
       "8             Python  \n",
       "9   Jupyter Notebook  \n",
       "10               C++  \n",
       "11        JavaScript  \n",
       "12        TypeScript  \n",
       "13        JavaScript  \n",
       "14        JavaScript  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.naukri.com/data-science-recruiters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Designation=[]\n",
    "Company=[]\n",
    "Skills=[]\n",
    "they_hire_for=[]\n",
    "Location=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aakash Harit\n",
      "shravan Kumar Gaddam\n",
      "Talent Acquisition Executive\n",
      "Anik Agrawal\n",
      "MARSIAN Technologies LLP\n",
      "subhas patel\n",
      "Jitendra Singh\n",
      "Abhishek - Only Analytics Hiring - India and\n",
      "Institute for Financial Management and Resear\n",
      "Balu Ramesh\n",
      "Asif Lucknowi\n",
      "InstaFinancials\n",
      "Kalpana Dumpala\n",
      "Mubarak\n",
      "Kushal Rastogi\n",
      "Manisha Yadav\n",
      "Kapil Devang\n",
      "Riya Rajesh\n",
      "Mahesh Babu Channa\n",
      "Rashmi Bhattacharjee\n",
      "Faizan Kareem\n",
      "Rithika dadwal\n",
      "Azahar Shaikh\n",
      "Sandhya Khandagale\n",
      "Shaun Rao\n",
      "Manas\n",
      "kumar\n",
      "Sunil Vedula\n",
      "Rajat Kumar\n",
      "Jayanth N\n",
      "Prateek Kumar\n",
      "Amit Sharma\n",
      "Kanan\n",
      "Shashikant Chaudhary\n",
      "Brad\n",
      "Rutuja Pawar\n",
      "Madhusudhan Sridhar\n",
      "Ankit Sinha\n",
      "Gaurav Chouhan\n",
      "Rashi Kacker\n",
      "Ashwini\n",
      "Balaji Kolli\n",
      "Rajani Nagaraj\n",
      "ROHIT Kumar\n",
      "Amir Chowdhury\n",
      "Shailja Mishra\n",
      "Sunny Sharma\n",
      "Sagar Lama\n",
      "SREEDHAR\n",
      "Dr. S. Prasanna Devi\n"
     ]
    }
   ],
   "source": [
    "nm=driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")\n",
    "for i in nm:\n",
    "    print(i.text)\n",
    "    Name.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Manager\n",
      "Company Recruiter\n",
      "Recruitment Professional\n",
      "Company Recruiter\n",
      "Company HR\n",
      "Founder CEO\n",
      "Manager- Talent Acquisition\n",
      "Recruitment Lead Consultant\n",
      "Programme Manager\n",
      "HR Administrator\n",
      "Director\n",
      "Human Resource\n",
      "Executive Hiring\n",
      "Company HR\n",
      "Company HR\n",
      "HR Executive\n",
      "HR Manager\n",
      "Manager Talent Acquisition\n",
      "HR Team Lead\n",
      "HR Head\n",
      "HR MANAGER\n",
      "HR Recruiter\n",
      "Company Recruiter\n",
      "HR Recruiter\n",
      "Manager Human Resources\n",
      "Lead Talent acquisition\n",
      "Proprietor\n",
      "CEO\n",
      "Founder CEO\n",
      "Project Manager\n",
      "Head\n",
      "Consultant\n",
      "senior technology instructor\n",
      "HR Recruiter/HR Excutive\n",
      "Manager, Technical Recruiting\n",
      "Technical Recruiter\n",
      "Erp Implementer\n",
      "Head Analytics\n",
      "Chief Technical Officer\n",
      "Sr Product Manager\n",
      "Director Global Delivery\n",
      "Co Founder\n",
      "HR Manager\n",
      "Architect\n",
      "Managing Partner\n",
      "HR Manager\n",
      "Managing Director - HR\n",
      "Company HR Manager\n",
      "Recruitment Consultant\n",
      "Professor and Head\n"
     ]
    }
   ],
   "source": [
    "ds=driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")\n",
    "for i in ds:\n",
    "    print(i.text)\n",
    "    Designation.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aakash Harit\n",
      "Data Science Network\n",
      "shravan Kumar Gaddam\n",
      "Shore Infotech India Pvt. Ltd\n",
      "Talent Acquisition Executive\n",
      "XenonStack\n",
      "Anik Agrawal\n",
      "Enerlytics Software Solutions Pvt Ltd\n",
      "MARSIAN Technologies LLP\n",
      "MARSIAN Technologies LLP\n",
      "subhas patel\n",
      "LibraryXProject\n",
      "Jitendra Singh\n",
      "Compunnel Technology India Pvt. Ltd\n",
      "Abhishek - Only Analytics Hiring - India and\n",
      "Apidel Technologies Division of Transpower\n",
      "Institute for Financial Management and Resear\n",
      "IFMR\n",
      "Balu Ramesh\n",
      "Techvantage Systems Pvt Ltd\n",
      "Asif Lucknowi\n",
      "Weupskill- Live Wire India\n",
      "InstaFinancials\n",
      "CBL Data Science Private Limited\n",
      "Kalpana Dumpala\n",
      "Innominds Software\n",
      "Mubarak\n",
      "MoneyTap\n",
      "Kushal Rastogi\n",
      "QuantMagnum Technologies Pvt. Ltd.\n",
      "Manisha Yadav\n",
      "Easi Tax\n",
      "Kapil Devang\n",
      "BISP Solutions\n",
      "Riya Rajesh\n",
      "Novelworx Digital Solutions\n",
      "Mahesh Babu Channa\n",
      "SocialPrachar.com\n",
      "Rashmi Bhattacharjee\n",
      "AXESTRACK SOFTWARE SOLUTIONS PRIVATE...\n",
      "Faizan Kareem\n",
      "FirstTech Consaltants Pvt.Ltd\n",
      "Rithika dadwal\n",
      "Affine Analytics\n",
      "Azahar Shaikh\n",
      "NEAL ANALYTICS SERVICES PVT LTD\n",
      "Sandhya Khandagale\n",
      "Compumatrice Multimedia Pvt Ltd\n",
      "Shaun Rao\n",
      "Exela Technologies\n",
      "Manas\n",
      "Autumn Leaf Consulting Services Private...\n",
      "kumar\n",
      "trainin\n",
      "Sunil Vedula\n",
      "Nanoprecise Sci Corp\n",
      "Rajat Kumar\n",
      "R.S Consultancy &amp; Services\n",
      "Jayanth N\n",
      "Dollarbird Information Services Pvt, Ltd\n",
      "Prateek Kumar\n",
      "Trisect\n",
      "Amit Sharma\n",
      "ASCO consulting\n",
      "Kanan\n",
      "NY INST\n",
      "Shashikant Chaudhary\n",
      "3D India Staffing Research &amp; Consulting...\n",
      "Brad\n",
      "O.C. Tanner\n",
      "Rutuja Pawar\n",
      "Demand Matrix\n",
      "Madhusudhan Sridhar\n",
      "MADHUSUDHAN SRIDHAR\n",
      "Ankit Sinha\n",
      "Suntech Global\n",
      "Gaurav Chouhan\n",
      "Strategic Consulting Lab\n",
      "Rashi Kacker\n",
      "Impel Labs Pvt. Ltd.\n",
      "Ashwini\n",
      "MRP Advisers\n",
      "Balaji Kolli\n",
      "Saras Solutions India Pvt Ltd\n",
      "Rajani Nagaraj\n",
      "WildJasmine\n",
      "ROHIT Kumar\n",
      "LNT Private Limited\n",
      "Amir Chowdhury\n",
      "Granular.ai\n",
      "Shailja Mishra\n",
      "Certybox Pvt.Ltd.\n",
      "Sunny Sharma\n",
      "Western Service Providers\n",
      "Sagar Lama\n",
      "AdView\n",
      "SREEDHAR\n",
      "JOBSMILL BUSINESS SOLUTIONS PRIVATE LIMITED\n",
      "Dr. S. Prasanna Devi\n",
      "SRM Institute of Science and Technology\n"
     ]
    }
   ],
   "source": [
    "cm=driver.find_elements_by_xpath(\"//a[@class='ellipsis']\")\n",
    "for i in cm:\n",
    "    print(i.text)\n",
    "    Company.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classic ASP Developer, Internet Marketing Professional, Data Science SME, Content Writers, SEO Professional, Revenue Professional\n",
      ".Net, Java, Data Science, Linux Administration, Sql Server Development, Winforms, Wcf Services, Wpf, Telecom Engineering, Technical Management, Software\n",
      "Web Designing, html5, Angular.js, seo, hadoop, Cloud Computing, python, Iphone Development, java, Machine Learning, aws, analytics, linux, puppet\n",
      "Mean Stack, javascript, angularjs, mongodb, Web Services, rest, express, Node.js, Big Data, iot, Data Science, Cloud Computing, saas, Aws\n",
      "Data Science, Artificial Intelligence, Machine Learning, Business Analytics\n",
      "Hadoop, Spark, Digital Strategy, Data Architecture, Command Center, Cdp, Dmp, Kafka, Data Science, Data Analysis, Big Data Analytics, Real Time Analysis, SQL\n",
      "Python, Data Science, .Net, Java, Big Data, Data Analytics, Business Intelligence, Msbi, Aws, Microsoft Azure, Devops\n",
      "Analytics, Business Intelligence, Business Analytics, Predictive Modeling, Predictive Analytics, Data Science, Data Analysis, Data Analytics, Big Data, Big\n",
      "Data Science\n",
      "Machine Learning, algorithms, Go Getter, Computer Science, spark, Big Data, hdfs, sql, cassandra, hadoop, python, scala, java, Data Science, Front End\n",
      "Technical Training, Software Development, Presentation Skills, B.tech, M.tech, B.e., mca, msc, Computer Science, freshers, jobs in indore, Data Science, itil\n",
      "Software Development, It Sales, Account Management, Data Analysis, Customer Service, Sr, Software Engineering, Mvc, Ajax, Asp.net, Html, C#, Javascript\n",
      "Qa, Ui/ux, Java Developer, Java Architect, C++/qt, Php, Lamp, Api, J2ee, Java, Soa, Esb, Middleware, Bigdata Achitect, Hadoop Architect, Deep\n",
      "Business Intelligence, Data Warehousing, Data Science, Business Analytics, Customer Support, Business Reporting, Bi\n",
      "Office Administration, Hr Administration, telecalling, client relationship management, Client Acquisition, Sales, Reception, HR, Recruitment, Onboarding, Human\n",
      "Telecalling, Client Interaction, Marketing, Research, Web Development, Social Media Marketing, Data Entry Operation, Excel, Ms Office, Invoicing\n",
      "Big Data, Hadoop, Data Analytics, Data Science\n",
      "Data Science\n",
      "Social Media, digital media maketing, seo, smm, smo, sem, Content Wirting, social media marketing, social media manager, digital media marketing manager\n",
      "Corporate Sales, Software Development, Software Sales, Marketing, Creative Designing, Corporate Planning, Senior Management, Crm, Client Relationship\n",
      "Data Analytics, Data Science, Machine Learning, Deep Learning, Nlp, Data Mining, Python, R, Database Administration, Text Mining\n",
      "Data Science, Machine Learning, Python, R, Deep Learning, Big Data, Hadoop\n",
      "Data Science, Artificial Intelligence, Machine Learning, Data Analytics\n",
      "Big Data, Data Science, Artificial Intelligence, Hadoop, Ui Development, Php, Freelancing, .Net, Software Testing, Sap, Leadership Hiring\n",
      "Java, Net, Angularjs, Hr, Infrastructure, Management, Project Management, Business Analysis, Data Science, Information Technology, Technology\n",
      "Software Architecture, Vp Engineering, Product Management, analytics, Data Science, Node.js, Principal Engineer, Big Data, python, angularjs, React.js\n",
      "Data Science, Hadoop, Rpas, Devops, Python, Aws, Teaching, Big Data\n",
      "Signal Processing, Machine Learning, Neural Networks, Data Science, Predictive Analytics, Time Series Analysis, Data Visualization, Technical Leadership, Data\n",
      "Web Technologies, Project Management, Software Architecture, Data Science, Object Oriented Programming, Computer Science, Electrical Engineering, Architecture\n",
      "Data Analytics, Managed Services, Team Leading, python, Machine Learning, Google Analytics, Dmp, Aws, Campaign Analytics, Digital Campaigns, Audience\n",
      "Java, Python, Angularjs, Software Testing, Machine Learning, Data Science, Javascript, Django, React.js, Node.js, Augmented Reality, Virtual Reality, Advanced\n",
      "Machine Learning, Artificial Intelligence, Data Science, Software Engineering, Software Development, Graduate Engineer Trainee, Fresher, Data Analytics, Java\n",
      "C, C++, Artificial Intelligence, Python, Php, Web Development, Matlab, Data Science, Augmented Reality, C C++\n",
      "Relationship Management, Retail Sales, Private Banking, Mutual Funds, NISM, Equity, Finance, Financial Products, Financial Services, Verbal, Written\n",
      "Data Science, Software Engineering\n",
      "Data Science, Big Data Analytics, Digital Marketing, Content Writing, Ui Development, Database Development, Qa Automation, Python, Project Management\n",
      "Data Science, Recruitment, Salary\n",
      "B.Tech, Tableau, Statistics, R, Analytics, Time Series, Data Science, Business Solutions, SQL, Technical Skills, SSAS, SQL Server, Analysis Services, Qlikview\n",
      "Software Development, Business Intelligence, Big Data Analytics, Database Administration, Data Science, Microsoft Azure, Spark, Cassandra, Object Oriented\n",
      "Data Science, Node.js, Angularjs\n",
      "Data Science, Media Marketing, Resource Planning, Managed Services, Display Advertising, Machine Learning, Python, Etl, Sql\n",
      "Data Analysis, Learning, Data Science, Computer Science, Communication Skills\n",
      "Java, Hadoop, R, Machine Learning, Spark, Flume, Hdfs, Data Mining, Sas, Big, Data Science, Cloudera, Impala, Bigdata\n",
      "Software Development, Core Java, Unit Testing, Customer Experience, Problem Solving, Communication Skills, Mysql, Data Science, Sales Management, Analytics\n",
      "Machine Learning, Data Science, Product Management, New Product, Data Analysis, Computer Vision, Deep Learning, Python, Remote Sensing\n",
      "consulting, Education Counseling, Educational Sales, Institutional Sales, pmp, Data Science, Business Development, Revenue Generation, Sales Achievement, new\n",
      "Software Professionals, Engineering, Technical Management, Financial Management, Human Resource Management, Banking, Google Adwords, Business Analysis, It\n",
      "Full Stack Developers, Product Manager, Data Science Engineer, Publishers Sales Manager\n",
      "Data Science, Machine Learning, Big Data Analytics, Spark, Python, R, Networking, Network Engineering, Placement, Training, Sql, Marketing, Mainframes, All\n",
      "Computer Science, Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Information Retrieval, Artificial Intelligence, Data Mining\n"
     ]
    }
   ],
   "source": [
    "sk=driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")\n",
    "for i in sk:\n",
    "    print(i.text)\n",
    "    Skills.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delhi\n",
      "Hyderabad / Secunderabad\n",
      "Chandigarh\n",
      "Ahmedabad\n",
      "Pune\n",
      "UK - (london)\n",
      "Delhi\n",
      "Vadodara / Baroda\n",
      "Chennai\n",
      "Trivandrum\n",
      "Indore\n",
      "Bengaluru / Bangalore\n",
      "Hyderabad / Secunderabad\n",
      "Bengaluru / Bangalore\n",
      "Mumbai\n",
      "Navi Mumbai\n",
      "Bhopal\n",
      "Cochin\n",
      "Hyderabad / Secunderabad\n",
      "Delhi\n",
      "Hyderabad / Secunderabad\n",
      "Pune\n",
      "Pune\n",
      "Pune\n",
      "Pune\n",
      "Bengaluru / Bangalore\n",
      "Bengaluru / Bangalore\n",
      "Delhi\n",
      "Mysoru / Mysore\n",
      "Noida\n",
      "New Delhi\n",
      "Chennai\n",
      "Aligarh\n",
      "Salt Lake City\n",
      "Pune\n",
      "Bengaluru / Bangalore\n",
      "Mumbai\n",
      "Indore\n",
      "Bengaluru / Bangalore\n",
      "MYSORE\n",
      "Hyderabad / Secunderabad\n",
      "Bengaluru / Bangalore\n",
      "Mumbai\n",
      "Noida\n",
      "Mumbai\n",
      "Bengaluru / Bangalore\n",
      "Hyderabad / Secunderabad\n",
      "Chennai\n"
     ]
    }
   ],
   "source": [
    "lc=driver.find_elements_by_xpath(\"//small[@class='ellipsis']\")\n",
    "for i in lc:\n",
    "    print(i.text)\n",
    "    Location.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Name\":Name[0:15],\n",
    "                \"Company\":Company[0:15],\n",
    "                \"Skills/they hired for\":Skills[0:15],\n",
    "                \"Location\":Location[0:15],\n",
    "                 \"Designation\":Designation[0:15]\n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills/they hired for</th>\n",
       "      <th>Location</th>\n",
       "      <th>Designation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>Classic ASP Developer, Internet Marketing Prof...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>HR Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>.Net, Java, Data Science, Linux Administration...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Company Recruiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Talent Acquisition Executive</td>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Web Designing, html5, Angular.js, seo, hadoop,...</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>Recruitment Professional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Shore Infotech India Pvt. Ltd</td>\n",
       "      <td>Mean Stack, javascript, angularjs, mongodb, We...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>Company Recruiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Talent Acquisition Executive</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Company HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>subhas patel</td>\n",
       "      <td>XenonStack</td>\n",
       "      <td>Hadoop, Spark, Digital Strategy, Data Architec...</td>\n",
       "      <td>UK - (london)</td>\n",
       "      <td>Founder CEO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jitendra Singh</td>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Python, Data Science, .Net, Java, Big Data, Da...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Manager- Talent Acquisition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Abhishek - Only Analytics Hiring - India and</td>\n",
       "      <td>Enerlytics Software Solutions Pvt Ltd</td>\n",
       "      <td>Analytics, Business Intelligence, Business Ana...</td>\n",
       "      <td>Vadodara / Baroda</td>\n",
       "      <td>Recruitment Lead Consultant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Institute for Financial Management and Resear</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>Programme Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Balu Ramesh</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Machine Learning, algorithms, Go Getter, Compu...</td>\n",
       "      <td>Trivandrum</td>\n",
       "      <td>HR Administrator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Asif Lucknowi</td>\n",
       "      <td>subhas patel</td>\n",
       "      <td>Technical Training, Software Development, Pres...</td>\n",
       "      <td>Indore</td>\n",
       "      <td>Director</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>InstaFinancials</td>\n",
       "      <td>LibraryXProject</td>\n",
       "      <td>Software Development, It Sales, Account Manage...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Human Resource</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kalpana Dumpala</td>\n",
       "      <td>Jitendra Singh</td>\n",
       "      <td>Qa, Ui/ux, Java Developer, Java Architect, C++...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "      <td>Executive Hiring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mubarak</td>\n",
       "      <td>Compunnel Technology India Pvt. Ltd</td>\n",
       "      <td>Business Intelligence, Data Warehousing, Data ...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "      <td>Company HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kushal Rastogi</td>\n",
       "      <td>Abhishek - Only Analytics Hiring - India and</td>\n",
       "      <td>Office Administration, Hr Administration, tele...</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Company HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  \\\n",
       "0                                    Aakash Harit   \n",
       "1                            shravan Kumar Gaddam   \n",
       "2                    Talent Acquisition Executive   \n",
       "3                                    Anik Agrawal   \n",
       "4                        MARSIAN Technologies LLP   \n",
       "5                                    subhas patel   \n",
       "6                                  Jitendra Singh   \n",
       "7    Abhishek - Only Analytics Hiring - India and   \n",
       "8   Institute for Financial Management and Resear   \n",
       "9                                     Balu Ramesh   \n",
       "10                                  Asif Lucknowi   \n",
       "11                                InstaFinancials   \n",
       "12                                Kalpana Dumpala   \n",
       "13                                        Mubarak   \n",
       "14                                 Kushal Rastogi   \n",
       "\n",
       "                                         Company  \\\n",
       "0                                   Aakash Harit   \n",
       "1                           Data Science Network   \n",
       "2                           shravan Kumar Gaddam   \n",
       "3                  Shore Infotech India Pvt. Ltd   \n",
       "4                   Talent Acquisition Executive   \n",
       "5                                     XenonStack   \n",
       "6                                   Anik Agrawal   \n",
       "7          Enerlytics Software Solutions Pvt Ltd   \n",
       "8                       MARSIAN Technologies LLP   \n",
       "9                       MARSIAN Technologies LLP   \n",
       "10                                  subhas patel   \n",
       "11                               LibraryXProject   \n",
       "12                                Jitendra Singh   \n",
       "13           Compunnel Technology India Pvt. Ltd   \n",
       "14  Abhishek - Only Analytics Hiring - India and   \n",
       "\n",
       "                                Skills/they hired for  \\\n",
       "0   Classic ASP Developer, Internet Marketing Prof...   \n",
       "1   .Net, Java, Data Science, Linux Administration...   \n",
       "2   Web Designing, html5, Angular.js, seo, hadoop,...   \n",
       "3   Mean Stack, javascript, angularjs, mongodb, We...   \n",
       "4   Data Science, Artificial Intelligence, Machine...   \n",
       "5   Hadoop, Spark, Digital Strategy, Data Architec...   \n",
       "6   Python, Data Science, .Net, Java, Big Data, Da...   \n",
       "7   Analytics, Business Intelligence, Business Ana...   \n",
       "8                                        Data Science   \n",
       "9   Machine Learning, algorithms, Go Getter, Compu...   \n",
       "10  Technical Training, Software Development, Pres...   \n",
       "11  Software Development, It Sales, Account Manage...   \n",
       "12  Qa, Ui/ux, Java Developer, Java Architect, C++...   \n",
       "13  Business Intelligence, Data Warehousing, Data ...   \n",
       "14  Office Administration, Hr Administration, tele...   \n",
       "\n",
       "                    Location                  Designation  \n",
       "0                      Delhi                   HR Manager  \n",
       "1   Hyderabad / Secunderabad            Company Recruiter  \n",
       "2                 Chandigarh     Recruitment Professional  \n",
       "3                  Ahmedabad            Company Recruiter  \n",
       "4                       Pune                   Company HR  \n",
       "5              UK - (london)                  Founder CEO  \n",
       "6                      Delhi  Manager- Talent Acquisition  \n",
       "7          Vadodara / Baroda  Recruitment Lead Consultant  \n",
       "8                    Chennai            Programme Manager  \n",
       "9                 Trivandrum             HR Administrator  \n",
       "10                    Indore                     Director  \n",
       "11     Bengaluru / Bangalore               Human Resource  \n",
       "12  Hyderabad / Secunderabad             Executive Hiring  \n",
       "13     Bengaluru / Bangalore                   Company HR  \n",
       "14                    Mumbai                   Company HR  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Book_name=[]\n",
    "Author_name=[]\n",
    "Volumes_sold=[]\n",
    "Publisher=[]\n",
    "Genre=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da Vinci Code,The\n",
      "Harry Potter and the Deathly Hallows\n",
      "Harry Potter and the Philosopher's Stone\n",
      "Harry Potter and the Order of the Phoenix\n",
      "Fifty Shades of Grey\n",
      "Harry Potter and the Goblet of Fire\n",
      "Harry Potter and the Chamber of Secrets\n",
      "Harry Potter and the Prisoner of Azkaban\n",
      "Angels and Demons\n",
      "Harry Potter and the Half-blood Prince:Children's Edition\n",
      "Fifty Shades Darker\n",
      "Twilight\n",
      "Girl with the Dragon Tattoo,The:Millennium Trilogy\n",
      "Fifty Shades Freed\n",
      "Lost Symbol,The\n",
      "New Moon\n",
      "Deception Point\n",
      "Eclipse\n",
      "Lovely Bones,The\n",
      "Curious Incident of the Dog in the Night-time,The\n",
      "Digital Fortress\n",
      "Short History of Nearly Everything,A\n",
      "Girl Who Played with Fire,The:Millennium Trilogy\n",
      "Breaking Dawn\n",
      "Very Hungry Caterpillar,The:The Very Hungry Caterpillar\n",
      "Gruffalo,The\n",
      "Jamie's 30-Minute Meals\n",
      "Kite Runner,The\n",
      "One Day\n",
      "Thousand Splendid Suns,A\n",
      "Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\n",
      "Time Traveler's Wife,The\n",
      "Atonement\n",
      "Bridget Jones's Diary:A Novel\n",
      "World According to Clarkson,The\n",
      "Captain Corelli's Mandolin\n",
      "Sound of Laughter,The\n",
      "Life of Pi\n",
      "Billy Connolly\n",
      "Child Called It,A\n",
      "Gruffalo's Child,The\n",
      "Angela's Ashes:A Memoir of a Childhood\n",
      "Birdsong\n",
      "Northern Lights:His Dark Materials S.\n",
      "Labyrinth\n",
      "Harry Potter and the Half-blood Prince\n",
      "Help,The\n",
      "Man and Boy\n",
      "Memoirs of a Geisha\n",
      "No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\n",
      "Island,The\n",
      "PS, I Love You\n",
      "You are What You Eat:The Plan That Will Change Your Life\n",
      "Shadow of the Wind,The\n",
      "Tales of Beedle the Bard,The\n",
      "Broker,The\n",
      "Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\n",
      "Subtle Knife,The:His Dark Materials S.\n",
      "Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation\n",
      "Delia's How to Cook:(Bk.1)\n",
      "Chocolat\n",
      "Boy in the Striped Pyjamas,The\n",
      "My Sister's Keeper\n",
      "Amber Spyglass,The:His Dark Materials S.\n",
      "To Kill a Mockingbird\n",
      "Men are from Mars, Women are from Venus:A Practical Guide for Improvin\n",
      "Dear Fatty\n",
      "Short History of Tractors in Ukrainian,A\n",
      "Hannibal\n",
      "Lord of the Rings,The\n",
      "Stupid White Men:...and Other Sorry Excuses for the State of the Natio\n",
      "Interpretation of Murder,The\n",
      "Sharon Osbourne Extreme:My Autobiography\n",
      "Alchemist,The:A Fable About Following Your Dream\n",
      "At My Mother's Knee ...:and Other Low Joints\n",
      "Notes from a Small Island\n",
      "Return of the Naked Chef,The\n",
      "Bridget Jones: The Edge of Reason\n",
      "Jamie's Italy\n",
      "I Can Make You Thin\n",
      "Down Under\n",
      "Summons,The\n",
      "Small Island\n",
      "Nigella Express\n",
      "Brick Lane\n",
      "Memory Keeper's Daughter,The\n",
      "Room on the Broom\n",
      "About a Boy\n",
      "My Booky Wook\n",
      "God Delusion,The\n",
      "\"Beano\" Annual,The\n",
      "White Teeth\n",
      "House at Riverton,The\n",
      "Book Thief,The\n",
      "Nights of Rain and Stars\n",
      "Ghost,The\n",
      "Happy Days with the Naked Chef\n",
      "Hunger Games,The:Hunger Games Trilogy\n",
      "Lost Boy,The:A Foster Child's Search for the Love of a Family\n",
      "Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\n"
     ]
    }
   ],
   "source": [
    "bn=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[2]\")\n",
    "for i in bn:\n",
    "    print(i.text)\n",
    "    Book_name.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown, Dan\n",
      "Rowling, J.K.\n",
      "Rowling, J.K.\n",
      "Rowling, J.K.\n",
      "James, E. L.\n",
      "Rowling, J.K.\n",
      "Rowling, J.K.\n",
      "Rowling, J.K.\n",
      "Brown, Dan\n",
      "Rowling, J.K.\n",
      "James, E. L.\n",
      "Meyer, Stephenie\n",
      "Larsson, Stieg\n",
      "James, E. L.\n",
      "Brown, Dan\n",
      "Meyer, Stephenie\n",
      "Brown, Dan\n",
      "Meyer, Stephenie\n",
      "Sebold, Alice\n",
      "Haddon, Mark\n",
      "Brown, Dan\n",
      "Bryson, Bill\n",
      "Larsson, Stieg\n",
      "Meyer, Stephenie\n",
      "Carle, Eric\n",
      "Donaldson, Julia\n",
      "Oliver, Jamie\n",
      "Hosseini, Khaled\n",
      "Nicholls, David\n",
      "Hosseini, Khaled\n",
      "Larsson, Stieg\n",
      "Niffenegger, Audrey\n",
      "McEwan, Ian\n",
      "Fielding, Helen\n",
      "Clarkson, Jeremy\n",
      "Bernieres, Louis de\n",
      "Kay, Peter\n",
      "Martel, Yann\n",
      "Stephenson, Pamela\n",
      "Pelzer, Dave\n",
      "Donaldson, Julia\n",
      "McCourt, Frank\n",
      "Faulks, Sebastian\n",
      "Pullman, Philip\n",
      "Mosse, Kate\n",
      "Rowling, J.K.\n",
      "Stockett, Kathryn\n",
      "Parsons, Tony\n",
      "Golden, Arthur\n",
      "McCall Smith, Alexander\n",
      "Hislop, Victoria\n",
      "Ahern, Cecelia\n",
      "McKeith, Gillian\n",
      "Zafon, Carlos Ruiz\n",
      "Rowling, J.K.\n",
      "Grisham, John\n",
      "Atkins, Robert C.\n",
      "Pullman, Philip\n",
      "Truss, Lynne\n",
      "Smith, Delia\n",
      "Harris, Joanne\n",
      "Boyne, John\n",
      "Picoult, Jodi\n",
      "Pullman, Philip\n",
      "Lee, Harper\n",
      "Gray, John\n",
      "French, Dawn\n",
      "Lewycka, Marina\n",
      "Harris, Thomas\n",
      "Tolkien, J. R. R.\n",
      "Moore, Michael\n",
      "Rubenfeld, Jed\n",
      "Osbourne, Sharon\n",
      "Coelho, Paulo\n",
      "O'Grady, Paul\n",
      "Bryson, Bill\n",
      "Oliver, Jamie\n",
      "Fielding, Helen\n",
      "Oliver, Jamie\n",
      "McKenna, Paul\n",
      "Bryson, Bill\n",
      "Grisham, John\n",
      "Levy, Andrea\n",
      "Lawson, Nigella\n",
      "Ali, Monica\n",
      "Edwards, Kim\n",
      "Donaldson, Julia\n",
      "Hornby, Nick\n",
      "Brand, Russell\n",
      "Dawkins, Richard\n",
      "0\n",
      "Smith, Zadie\n",
      "Morton, Kate\n",
      "Zusak, Markus\n",
      "Binchy, Maeve\n",
      "Harris, Robert\n",
      "Oliver, Jamie\n",
      "Collins, Suzanne\n",
      "Pelzer, Dave\n",
      "Oliver, Jamie\n"
     ]
    }
   ],
   "source": [
    "an=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[3]\")\n",
    "for i in an:\n",
    "    print(i.text)\n",
    "    Author_name.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,094,805\n",
      "4,475,152\n",
      "4,200,654\n",
      "4,179,479\n",
      "3,758,936\n",
      "3,583,215\n",
      "3,484,047\n",
      "3,377,906\n",
      "3,193,946\n",
      "2,950,264\n",
      "2,479,784\n",
      "2,315,405\n",
      "2,233,570\n",
      "2,193,928\n",
      "2,183,031\n",
      "2,152,737\n",
      "2,062,145\n",
      "2,052,876\n",
      "2,005,598\n",
      "1,979,552\n",
      "1,928,900\n",
      "1,852,919\n",
      "1,814,784\n",
      "1,787,118\n",
      "1,783,535\n",
      "1,781,269\n",
      "1,743,266\n",
      "1,629,119\n",
      "1,616,068\n",
      "1,583,992\n",
      "1,555,135\n",
      "1,546,886\n",
      "1,539,428\n",
      "1,508,205\n",
      "1,489,403\n",
      "1,352,318\n",
      "1,310,207\n",
      "1,310,176\n",
      "1,231,957\n",
      "1,217,712\n",
      "1,208,711\n",
      "1,204,058\n",
      "1,184,967\n",
      "1,181,503\n",
      "1,181,093\n",
      "1,153,181\n",
      "1,132,336\n",
      "1,130,802\n",
      "1,126,337\n",
      "1,115,549\n",
      "1,108,328\n",
      "1,107,379\n",
      "1,104,403\n",
      "1,092,349\n",
      "1,090,847\n",
      "1,087,262\n",
      "1,054,196\n",
      "1,037,160\n",
      "1,023,688\n",
      "1,015,956\n",
      "1,009,873\n",
      "1,004,414\n",
      "1,003,780\n",
      "1,002,314\n",
      "998,213\n",
      "992,846\n",
      "986,753\n",
      "986,115\n",
      "970,509\n",
      "967,466\n",
      "963,353\n",
      "962,515\n",
      "959,496\n",
      "956,114\n",
      "945,640\n",
      "931,312\n",
      "925,425\n",
      "924,695\n",
      "906,968\n",
      "905,086\n",
      "890,847\n",
      "869,671\n",
      "869,659\n",
      "862,602\n",
      "856,540\n",
      "845,858\n",
      "842,535\n",
      "828,215\n",
      "820,563\n",
      "816,907\n",
      "816,585\n",
      "815,586\n",
      "814,370\n",
      "809,641\n",
      "808,900\n",
      "807,311\n",
      "794,201\n",
      "792,187\n",
      "791,507\n",
      "791,095\n"
     ]
    }
   ],
   "source": [
    "vs=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[4]\")\n",
    "for i in vs:\n",
    "    print(i.text)\n",
    "    Volumes_sold.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transworld\n",
      "Bloomsbury\n",
      "Bloomsbury\n",
      "Bloomsbury\n",
      "Random House\n",
      "Bloomsbury\n",
      "Bloomsbury\n",
      "Bloomsbury\n",
      "Transworld\n",
      "Bloomsbury\n",
      "Random House\n",
      "Little, Brown Book\n",
      "Quercus\n",
      "Random House\n",
      "Transworld\n",
      "Little, Brown Book\n",
      "Transworld\n",
      "Little, Brown Book\n",
      "Pan Macmillan\n",
      "Random House\n",
      "Transworld\n",
      "Transworld\n",
      "Quercus\n",
      "Little, Brown Book\n",
      "Penguin\n",
      "Pan Macmillan\n",
      "Penguin\n",
      "Bloomsbury\n",
      "Hodder & Stoughton\n",
      "Bloomsbury\n",
      "Quercus\n",
      "Random House\n",
      "Random House\n",
      "Pan Macmillan\n",
      "Penguin\n",
      "Random House\n",
      "Random House\n",
      "Canongate\n",
      "HarperCollins\n",
      "Orion\n",
      "Pan Macmillan\n",
      "HarperCollins\n",
      "Random House\n",
      "Scholastic Ltd.\n",
      "Orion\n",
      "Bloomsbury\n",
      "Penguin\n",
      "HarperCollins\n",
      "Random House\n",
      "Little, Brown Book\n",
      "Headline\n",
      "HarperCollins\n",
      "Penguin\n",
      "Orion\n",
      "Bloomsbury\n",
      "Random House\n",
      "Random House\n",
      "Scholastic Ltd.\n",
      "Profile Books Group\n",
      "Random House\n",
      "Transworld\n",
      "Random House Childrens Books G\n",
      "Hodder & Stoughton\n",
      "Scholastic Ltd.\n",
      "Random House\n",
      "HarperCollins\n",
      "Random House\n",
      "Penguin\n",
      "Random House\n",
      "HarperCollins\n",
      "Penguin\n",
      "Headline\n",
      "Little, Brown Book\n",
      "HarperCollins\n",
      "Transworld\n",
      "Transworld\n",
      "Penguin\n",
      "Pan Macmillan\n",
      "Penguin\n",
      "Transworld\n",
      "Transworld\n",
      "Random House\n",
      "Headline\n",
      "Random House\n",
      "Transworld\n",
      "Penguin\n",
      "Pan Macmillan\n",
      "Penguin\n",
      "Hodder & Stoughton\n",
      "Transworld\n",
      "D.C. Thomson\n",
      "Penguin\n",
      "Pan Macmillan\n",
      "Transworld\n",
      "Orion\n",
      "Random House\n",
      "Penguin\n",
      "Scholastic Ltd.\n",
      "Orion\n",
      "Penguin\n"
     ]
    }
   ],
   "source": [
    "pp=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[5]\")\n",
    "for i in pp:\n",
    "    print(i.text)\n",
    "    Publisher.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime, Thriller & Adventure\n",
      "Children's Fiction\n",
      "Children's Fiction\n",
      "Children's Fiction\n",
      "Romance & Sagas\n",
      "Children's Fiction\n",
      "Children's Fiction\n",
      "Children's Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Children's Fiction\n",
      "Romance & Sagas\n",
      "Young Adult Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Romance & Sagas\n",
      "Crime, Thriller & Adventure\n",
      "Young Adult Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Young Adult Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Popular Science\n",
      "Crime, Thriller & Adventure\n",
      "Young Adult Fiction\n",
      "Picture Books\n",
      "Picture Books\n",
      "Food & Drink: General\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Crime, Thriller & Adventure\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Humour: Collections & General\n",
      "General & Literary Fiction\n",
      "Autobiography: General\n",
      "General & Literary Fiction\n",
      "Biography: The Arts\n",
      "Autobiography: General\n",
      "Picture Books\n",
      "Autobiography: General\n",
      "General & Literary Fiction\n",
      "Young Adult Fiction\n",
      "General & Literary Fiction\n",
      "Science Fiction & Fantasy\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Crime, Thriller & Adventure\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Fitness & Diet\n",
      "General & Literary Fiction\n",
      "Children's Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Fitness & Diet\n",
      "Young Adult Fiction\n",
      "Usage & Writing Guides\n",
      "Food & Drink: General\n",
      "General & Literary Fiction\n",
      "Young Adult Fiction\n",
      "General & Literary Fiction\n",
      "Young Adult Fiction\n",
      "General & Literary Fiction\n",
      "Popular Culture & Media: General Interest\n",
      "Autobiography: The Arts\n",
      "General & Literary Fiction\n",
      "Crime, Thriller & Adventure\n",
      "Science Fiction & Fantasy\n",
      "Current Affairs & Issues\n",
      "Crime, Thriller & Adventure\n",
      "Autobiography: The Arts\n",
      "General & Literary Fiction\n",
      "Autobiography: The Arts\n",
      "Travel Writing\n",
      "Food & Drink: General\n",
      "General & Literary Fiction\n",
      "National & Regional Cuisine\n",
      "Fitness & Diet\n",
      "Travel Writing\n",
      "Crime, Thriller & Adventure\n",
      "General & Literary Fiction\n",
      "Food & Drink: General\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Picture Books\n",
      "General & Literary Fiction\n",
      "Autobiography: The Arts\n",
      "Popular Science\n",
      "Children's Annuals\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "General & Literary Fiction\n",
      "Food & Drink: General\n",
      "Young Adult Fiction\n",
      "Biography: General\n",
      "Food & Drink: General\n"
     ]
    }
   ],
   "source": [
    "gg=driver.find_elements_by_xpath(\"//div[@class='embed block']/table/tbody/tr/td[6]\")\n",
    "for i in gg:\n",
    "    print(i.text)\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Name\":Book_name[0:15],\n",
    "                \"Author\":Author_name[0:15],\n",
    "                \"volumes sold\":Volumes_sold[0:15],\n",
    "                \"Publisher\":Publisher[0:15],\n",
    "                 \"Genre\":Genre[0:15]\n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Author</th>\n",
       "      <th>volumes sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harry Potter and the Goblet of Fire</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>3,583,215</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Harry Potter and the Chamber of Secrets</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>3,484,047</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Harry Potter and the Prisoner of Azkaban</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>3,377,906</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Angels and Demons</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>3,193,946</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Harry Potter and the Half-blood Prince:Childre...</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>2,950,264</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fifty Shades Darker</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>2,479,784</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Twilight</td>\n",
       "      <td>Meyer, Stephenie</td>\n",
       "      <td>2,315,405</td>\n",
       "      <td>Little, Brown Book</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Girl with the Dragon Tattoo,The:Millennium Tri...</td>\n",
       "      <td>Larsson, Stieg</td>\n",
       "      <td>2,233,570</td>\n",
       "      <td>Quercus</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Fifty Shades Freed</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>2,193,928</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lost Symbol,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>2,183,031</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Name            Author  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "5                 Harry Potter and the Goblet of Fire     Rowling, J.K.   \n",
       "6             Harry Potter and the Chamber of Secrets     Rowling, J.K.   \n",
       "7            Harry Potter and the Prisoner of Azkaban     Rowling, J.K.   \n",
       "8                                   Angels and Demons        Brown, Dan   \n",
       "9   Harry Potter and the Half-blood Prince:Childre...     Rowling, J.K.   \n",
       "10                                Fifty Shades Darker      James, E. L.   \n",
       "11                                           Twilight  Meyer, Stephenie   \n",
       "12  Girl with the Dragon Tattoo,The:Millennium Tri...    Larsson, Stieg   \n",
       "13                                 Fifty Shades Freed      James, E. L.   \n",
       "14                                    Lost Symbol,The        Brown, Dan   \n",
       "\n",
       "   volumes sold           Publisher                        Genre  \n",
       "0     5,094,805          Transworld  Crime, Thriller & Adventure  \n",
       "1     4,475,152          Bloomsbury           Children's Fiction  \n",
       "2     4,200,654          Bloomsbury           Children's Fiction  \n",
       "3     4,179,479          Bloomsbury           Children's Fiction  \n",
       "4     3,758,936        Random House              Romance & Sagas  \n",
       "5     3,583,215          Bloomsbury           Children's Fiction  \n",
       "6     3,484,047          Bloomsbury           Children's Fiction  \n",
       "7     3,377,906          Bloomsbury           Children's Fiction  \n",
       "8     3,193,946          Transworld  Crime, Thriller & Adventure  \n",
       "9     2,950,264          Bloomsbury           Children's Fiction  \n",
       "10    2,479,784        Random House              Romance & Sagas  \n",
       "11    2,315,405  Little, Brown Book          Young Adult Fiction  \n",
       "12    2,233,570             Quercus  Crime, Thriller & Adventure  \n",
       "13    2,193,928        Random House              Romance & Sagas  \n",
       "14    2,183,031          Transworld  Crime, Thriller & Adventure  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.imdb.com/list/ls095964455/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name=[]\n",
    "Year_span=[]\n",
    "Genre=[]\n",
    "Run_time=[]\n",
    "Ratings=[]\n",
    "Votes=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game of Thrones\n",
      "Stranger Things\n",
      "The Walking Dead\n",
      "13 Reasons Why\n",
      "The 100\n",
      "Orange Is the New Black\n",
      "Riverdale\n",
      "Grey's Anatomy\n",
      "The Flash\n",
      "Arrow\n",
      "Money Heist\n",
      "The Big Bang Theory\n",
      "Black Mirror\n",
      "Sherlock\n",
      "Vikings\n",
      "Pretty Little Liars\n",
      "The Vampire Diaries\n",
      "American Horror Story\n",
      "Breaking Bad\n",
      "Lucifer\n",
      "Supernatural\n",
      "Prison Break\n",
      "How to Get Away with Murder\n",
      "Teen Wolf\n",
      "The Simpsons\n",
      "Once Upon a Time\n",
      "Narcos\n",
      "Daredevil\n",
      "Friends\n",
      "How I Met Your Mother\n",
      "Suits\n",
      "Mr. Robot\n",
      "The Originals\n",
      "Supergirl\n",
      "Gossip Girl\n",
      "Sense8\n",
      "Gotham\n",
      "Westworld\n",
      "Jessica Jones\n",
      "Modern Family\n",
      "Rick and Morty\n",
      "Shadowhunters\n",
      "The End of the F***ing World\n",
      "House of Cards\n",
      "Dark\n",
      "Elite\n",
      "Sex Education\n",
      "Shameless\n",
      "New Girl\n",
      "Agents of S.H.I.E.L.D.\n",
      "You\n",
      "Dexter\n",
      "Fear the Walking Dead\n",
      "Family Guy\n",
      "The Blacklist\n",
      "Lost\n",
      "Peaky Blinders\n",
      "House\n",
      "Quantico\n",
      "Orphan Black\n",
      "Homeland\n",
      "Blindspot\n",
      "DC's Legends of Tomorrow\n",
      "The Handmaid's Tale\n",
      "Chilling Adventures of Sabrina\n",
      "The Good Doctor\n",
      "Jane the Virgin\n",
      "Glee\n",
      "South Park\n",
      "Brooklyn Nine-Nine\n",
      "Under the Dome\n",
      "The Umbrella Academy\n",
      "True Detective\n",
      "The OA\n",
      "Desperate Housewives\n",
      "Better Call Saul\n",
      "Bates Motel\n",
      "The Punisher\n",
      "Atypical\n",
      "Dynasty\n",
      "This Is Us\n",
      "The Good Place\n",
      "Iron Fist\n",
      "The Rain\n",
      "Mindhunter\n",
      "Revenge\n",
      "Luke Cage\n",
      "Scandal\n",
      "The Defenders\n",
      "Big Little Lies\n",
      "Insatiable\n",
      "The Mentalist\n",
      "The Crown\n",
      "Chernobyl\n",
      "iZombie\n",
      "Reign\n",
      "A Series of Unfortunate Events\n",
      "Criminal Minds\n",
      "Scream: The TV Series\n",
      "The Haunting of Hill House\n"
     ]
    }
   ],
   "source": [
    "nm=driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\")\n",
    "for i in nm:\n",
    "    print(i.text)\n",
    "    Name.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2011–2019)\n",
      "(2016– )\n",
      "(2010– )\n",
      "(2017–2020)\n",
      "(2014–2020)\n",
      "(2013–2019)\n",
      "(2017– )\n",
      "(2005– )\n",
      "(2014– )\n",
      "(2012–2020)\n",
      "(2017– )\n",
      "(2007–2019)\n",
      "(2011– )\n",
      "(2010–2017)\n",
      "(2013–2020)\n",
      "(2010–2017)\n",
      "(2009–2017)\n",
      "(2011– )\n",
      "(2008–2013)\n",
      "(2016– )\n",
      "(2005–2020)\n",
      "(2005–2017)\n",
      "(2014–2020)\n",
      "(2011–2017)\n",
      "(1989– )\n",
      "(2011–2018)\n",
      "(2015–2017)\n",
      "(2015–2018)\n",
      "(1994–2004)\n",
      "(2005–2014)\n",
      "(2011–2019)\n",
      "(2015–2019)\n",
      "(2013–2018)\n",
      "(2015–2021)\n",
      "(2007–2012)\n",
      "(2015–2018)\n",
      "(2014–2019)\n",
      "(2016– )\n",
      "(2015–2019)\n",
      "(2009–2020)\n",
      "(2013– )\n",
      "(2016–2019)\n",
      "(2017–2019)\n",
      "(2013–2018)\n",
      "(2017–2020)\n",
      "(2018– )\n",
      "(2019– )\n",
      "(2011–2021)\n",
      "(2011–2018)\n",
      "(2013–2020)\n",
      "(2018– )\n",
      "(2006–2021)\n",
      "(2015– )\n",
      "(1999– )\n",
      "(2013– )\n",
      "(2004–2010)\n",
      "(2013– )\n",
      "(2004–2012)\n",
      "(2015–2018)\n",
      "(2013–2017)\n",
      "(2011–2020)\n",
      "(2015–2020)\n",
      "(2016– )\n",
      "(2017– )\n",
      "(2018–2020)\n",
      "(2017– )\n",
      "(2014–2019)\n",
      "(2009–2015)\n",
      "(1997– )\n",
      "(2013– )\n",
      "(2013–2015)\n",
      "(2019– )\n",
      "(2014– )\n",
      "(2016–2019)\n",
      "(2004–2012)\n",
      "(2015–2021)\n",
      "(2013–2017)\n",
      "(2017–2019)\n",
      "(2017–2021)\n",
      "(2017– )\n",
      "(2016– )\n",
      "(2016–2020)\n",
      "(2017–2018)\n",
      "(2018–2020)\n",
      "(2017–2019)\n",
      "(2011–2015)\n",
      "(2016–2018)\n",
      "(2012–2018)\n",
      "(2017)\n",
      "(2017–2019)\n",
      "(2018–2019)\n",
      "(2008–2015)\n",
      "(2016– )\n",
      "(2019)\n",
      "(2015–2019)\n",
      "(2013–2017)\n",
      "(2017–2019)\n",
      "(2005–2020)\n",
      "(2015–2019)\n",
      "(2018)\n"
     ]
    }
   ],
   "source": [
    "ys=driver.find_elements_by_xpath(\"//span[@class='lister-item-year text-muted unbold']\")\n",
    "for i in ys:\n",
    "    print(i.text)\n",
    "    Year_span.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action, Adventure, Drama\n",
      "Drama, Fantasy, Horror\n",
      "Drama, Horror, Thriller\n",
      "Drama, Mystery, Thriller\n",
      "Drama, Mystery, Sci-Fi\n",
      "Comedy, Crime, Drama\n",
      "Crime, Drama, Mystery\n",
      "Drama, Romance\n",
      "Action, Adventure, Drama\n",
      "Action, Adventure, Crime\n",
      "Action, Crime, Mystery\n",
      "Comedy, Romance\n",
      "Drama, Sci-Fi, Thriller\n",
      "Crime, Drama, Mystery\n",
      "Action, Adventure, Drama\n",
      "Drama, Mystery, Romance\n",
      "Drama, Fantasy, Horror\n",
      "Drama, Horror, Thriller\n",
      "Crime, Drama, Thriller\n",
      "Crime, Drama, Fantasy\n",
      "Drama, Fantasy, Horror\n",
      "Action, Crime, Drama\n",
      "Crime, Drama, Mystery\n",
      "Action, Drama, Fantasy\n",
      "Animation, Comedy\n",
      "Adventure, Fantasy, Romance\n",
      "Biography, Crime, Drama\n",
      "Action, Crime, Drama\n",
      "Comedy, Romance\n",
      "Comedy, Romance\n",
      "Comedy, Drama\n",
      "Crime, Drama, Thriller\n",
      "Drama, Fantasy, Horror\n",
      "Action, Adventure, Drama\n",
      "Drama, Romance\n",
      "Drama, Mystery, Sci-Fi\n",
      "Action, Crime, Drama\n",
      "Drama, Mystery, Sci-Fi\n",
      "Action, Crime, Drama\n",
      "Comedy, Drama, Romance\n",
      "Animation, Adventure, Comedy\n",
      "Action, Drama, Fantasy\n",
      "Adventure, Comedy, Crime\n",
      "Drama\n",
      "Crime, Drama, Mystery\n",
      "Crime, Drama, Thriller\n",
      "Comedy, Drama\n",
      "Comedy, Drama\n",
      "Comedy\n",
      "Action, Adventure, Drama\n",
      "Crime, Drama, Romance\n",
      "Crime, Drama, Mystery\n",
      "Drama, Horror, Sci-Fi\n",
      "Animation, Comedy\n",
      "Crime, Drama, Mystery\n",
      "Adventure, Drama, Fantasy\n",
      "Crime, Drama\n",
      "Drama, Mystery\n",
      "Crime, Drama, Mystery\n",
      "Action, Drama, Sci-Fi\n",
      "Crime, Drama, Mystery\n",
      "Action, Crime, Drama\n",
      "Action, Adventure, Drama\n",
      "Drama, Sci-Fi, Thriller\n",
      "Drama, Fantasy, Horror\n",
      "Drama\n",
      "Comedy\n",
      "Comedy, Drama, Music\n",
      "Animation, Comedy\n",
      "Comedy, Crime\n",
      "Drama, Mystery, Sci-Fi\n",
      "Action, Adventure, Comedy\n",
      "Crime, Drama, Mystery\n",
      "Drama, Fantasy, Mystery\n",
      "Comedy, Drama, Mystery\n",
      "Crime, Drama\n",
      "Drama, Horror, Mystery\n",
      "Action, Crime, Drama\n",
      "Comedy, Drama\n",
      "Drama\n",
      "Comedy, Drama, Romance\n",
      "Comedy, Drama, Fantasy\n",
      "Action, Adventure, Crime\n",
      "Drama, Sci-Fi, Thriller\n",
      "Crime, Drama, Thriller\n",
      "Drama, Mystery, Thriller\n",
      "Action, Crime, Drama\n",
      "Drama, Thriller\n",
      "Action, Adventure, Crime\n",
      "Crime, Drama, Mystery\n",
      "Comedy, Drama, Thriller\n",
      "Crime, Drama, Mystery\n",
      "Biography, Drama, History\n",
      "Drama, History, Thriller\n",
      "Comedy, Crime, Drama\n",
      "Drama, Fantasy\n",
      "Adventure, Comedy, Drama\n",
      "Crime, Drama, Mystery\n",
      "Crime, Drama, Horror\n",
      "Drama, Horror, Mystery\n"
     ]
    }
   ],
   "source": [
    "gn=driver.find_elements_by_xpath(\"//span[@class='genre']\")\n",
    "for i in gn:\n",
    "    print(i.text)\n",
    "    Genre.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 min\n",
      "51 min\n",
      "44 min\n",
      "60 min\n",
      "43 min\n",
      "59 min\n",
      "45 min\n",
      "41 min\n",
      "43 min\n",
      "42 min\n",
      "70 min\n",
      "22 min\n",
      "60 min\n",
      "88 min\n",
      "44 min\n",
      "44 min\n",
      "43 min\n",
      "60 min\n",
      "49 min\n",
      "42 min\n",
      "44 min\n",
      "44 min\n",
      "43 min\n",
      "41 min\n",
      "22 min\n",
      "60 min\n",
      "49 min\n",
      "54 min\n",
      "22 min\n",
      "22 min\n",
      "44 min\n",
      "49 min\n",
      "45 min\n",
      "43 min\n",
      "42 min\n",
      "60 min\n",
      "42 min\n",
      "62 min\n",
      "56 min\n",
      "22 min\n",
      "23 min\n",
      "42 min\n",
      "25 min\n",
      "51 min\n",
      "60 min\n",
      "60 min\n",
      "45 min\n",
      "46 min\n",
      "22 min\n",
      "45 min\n",
      "45 min\n",
      "53 min\n",
      "44 min\n",
      "22 min\n",
      "43 min\n",
      "44 min\n",
      "60 min\n",
      "44 min\n",
      "42 min\n",
      "44 min\n",
      "55 min\n",
      "42 min\n",
      "42 min\n",
      "60 min\n",
      "60 min\n",
      "41 min\n",
      "60 min\n",
      "44 min\n",
      "22 min\n",
      "22 min\n",
      "43 min\n",
      "60 min\n",
      "55 min\n",
      "60 min\n",
      "45 min\n",
      "46 min\n",
      "45 min\n",
      "53 min\n",
      "30 min\n",
      "42 min\n",
      "45 min\n",
      "22 min\n",
      "55 min\n",
      "45 min\n",
      "60 min\n",
      "44 min\n",
      "55 min\n",
      "43 min\n",
      "50 min\n",
      "60 min\n",
      "45 min\n",
      "43 min\n",
      "58 min\n",
      "330 min\n",
      "42 min\n",
      "42 min\n",
      "50 min\n",
      "42 min\n",
      "45 min\n",
      "572 min\n"
     ]
    }
   ],
   "source": [
    "rt=driver.find_elements_by_xpath(\"//span[@class='runtime']\")\n",
    "for i in rt:\n",
    "    print(i.text)\n",
    "    Run_time.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3\n",
      "8.7\n",
      "8.2\n",
      "7.6\n",
      "7.6\n",
      "8.1\n",
      "6.9\n",
      "7.6\n",
      "7.7\n",
      "7.5\n",
      "8.3\n",
      "8.1\n",
      "8.8\n",
      "9.1\n",
      "8.5\n",
      "7.4\n",
      "7.7\n",
      "8\n",
      "9.5\n",
      "8.1\n",
      "8.4\n",
      "8.3\n",
      "8.1\n",
      "7.6\n",
      "8.7\n",
      "7.7\n",
      "8.8\n",
      "8.6\n",
      "8.9\n",
      "8.3\n",
      "8.5\n",
      "8.6\n",
      "8.2\n",
      "6.3\n",
      "7.4\n",
      "8.3\n",
      "7.8\n",
      "8.6\n",
      "7.9\n",
      "8.4\n",
      "9.2\n",
      "6.6\n",
      "8.1\n",
      "8.7\n",
      "8.8\n",
      "7.6\n",
      "8.3\n",
      "8.6\n",
      "7.7\n",
      "7.5\n",
      "7.7\n",
      "8.6\n",
      "6.9\n",
      "8.1\n",
      "8\n",
      "8.3\n",
      "8.8\n",
      "8.7\n",
      "6.7\n",
      "8.3\n",
      "8.3\n",
      "7.4\n",
      "6.8\n",
      "8.4\n",
      "7.5\n",
      "8.2\n",
      "7.8\n",
      "6.7\n",
      "8.7\n",
      "8.4\n",
      "6.6\n",
      "8\n",
      "9\n",
      "7.8\n",
      "7.5\n",
      "8.7\n",
      "8.2\n",
      "8.5\n",
      "8.3\n",
      "7.3\n",
      "8.7\n",
      "8.2\n",
      "6.5\n",
      "6.3\n",
      "8.6\n",
      "7.8\n",
      "7.3\n",
      "7.7\n",
      "7.3\n",
      "8.5\n",
      "6.5\n",
      "8.1\n",
      "8.7\n",
      "9.4\n",
      "7.8\n",
      "7.5\n",
      "7.8\n",
      "8.1\n",
      "7.2\n",
      "8.6\n"
     ]
    }
   ],
   "source": [
    "rr=driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']\")\n",
    "for i in rr:\n",
    "    print(i.text)\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|\n",
      "1,768,795\n",
      "|\n",
      "821,278\n",
      "|\n",
      "852,923\n",
      "|\n",
      "256,067\n",
      "|\n",
      "216,277\n",
      "|\n",
      "278,832\n",
      "|\n",
      "117,803\n",
      "|\n",
      "250,713\n",
      "|\n",
      "305,151\n",
      "|\n",
      "407,531\n",
      "|\n",
      "297,683\n",
      "|\n",
      "722,841\n",
      "|\n",
      "444,127\n",
      "|\n",
      "806,744\n",
      "|\n",
      "431,697\n",
      "|\n",
      "152,203\n",
      "|\n",
      "282,857\n",
      "|\n",
      "277,363\n",
      "|\n",
      "1,463,764\n",
      "|\n",
      "232,131\n",
      "|\n",
      "392,411\n",
      "|\n",
      "476,095\n",
      "|\n",
      "129,184\n",
      "|\n",
      "126,683\n",
      "|\n",
      "365,514\n",
      "|\n",
      "208,513\n",
      "|\n",
      "357,306\n",
      "|\n",
      "362,371\n",
      "|\n",
      "826,899\n",
      "|\n",
      "602,478\n",
      "|\n",
      "363,389\n",
      "|\n",
      "333,157\n",
      "|\n",
      "117,439\n",
      "|\n",
      "111,119\n",
      "|\n",
      "154,283\n",
      "|\n",
      "139,353\n",
      "|\n",
      "211,790\n",
      "|\n",
      "427,892\n",
      "|\n",
      "193,146\n",
      "|\n",
      "354,914\n",
      "|\n",
      "375,398\n",
      "|\n",
      "53,262\n",
      "|\n",
      "145,815\n",
      "|\n",
      "466,731\n",
      "|\n",
      "283,350\n",
      "|\n",
      "48,092\n",
      "|\n",
      "157,902\n",
      "|\n",
      "203,016\n",
      "|\n",
      "192,221\n",
      "|\n",
      "199,676\n",
      "|\n",
      "143,307\n",
      "|\n",
      "646,137\n",
      "|\n",
      "112,397\n",
      "|\n",
      "304,521\n",
      "|\n",
      "197,011\n",
      "|\n",
      "495,348\n",
      "|\n",
      "347,764\n",
      "|\n",
      "414,506\n",
      "|\n",
      "57,194\n",
      "|\n",
      "101,340\n",
      "|\n",
      "312,635\n",
      "|\n",
      "66,621\n",
      "|\n",
      "91,922\n",
      "|\n",
      "171,925\n",
      "|\n",
      "77,413\n",
      "|\n",
      "62,784\n",
      "|\n",
      "38,729\n",
      "|\n",
      "136,675\n",
      "|\n",
      "331,396\n",
      "|\n",
      "225,004\n",
      "|\n",
      "101,158\n",
      "|\n",
      "158,724\n",
      "|\n",
      "499,127\n",
      "|\n",
      "90,511\n",
      "|\n",
      "115,608\n",
      "|\n",
      "323,549\n",
      "|\n",
      "96,944\n",
      "|\n",
      "184,223\n",
      "|\n",
      "60,191\n",
      "|\n",
      "14,813\n",
      "|\n",
      "106,316\n",
      "|\n",
      "114,832\n",
      "|\n",
      "114,679\n",
      "|\n",
      "30,782\n",
      "|\n",
      "216,808\n",
      "|\n",
      "112,505\n",
      "|\n",
      "116,779\n",
      "|\n",
      "67,239\n",
      "|\n",
      "90,752\n",
      "|\n",
      "156,343\n",
      "|\n",
      "22,554\n",
      "|\n",
      "165,419\n",
      "|\n",
      "149,111\n",
      "|\n",
      "540,746\n",
      "|\n",
      "60,447\n",
      "|\n",
      "43,557\n",
      "|\n",
      "53,847\n",
      "|\n",
      "161,059\n",
      "|\n",
      "34,079\n",
      "|\n",
      "182,139\n"
     ]
    }
   ],
   "source": [
    "vt=driver.find_elements_by_xpath(\"//p[@class='text-muted text-small']/span[2]\")\n",
    "for i in vt:\n",
    "    print(i.text)\n",
    "    Votes.append(i.text)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Name\":Name[0:15],\n",
    "                \"Year\":Year_span[0:15],\n",
    "                \"Genre\":Genre[0:15],\n",
    "                \"time\":Run_time[0:15],\n",
    "                 \"Rating\":Ratings[0:15],\n",
    "                 \"vote\":Votes[0:15]\n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>time</th>\n",
       "      <th>Rating</th>\n",
       "      <th>vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.3</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,768,795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010– )</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>821,278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Orange Is the New Black</td>\n",
       "      <td>(2013–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>59 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>852,923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Riverdale</td>\n",
       "      <td>(2017– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>6.9</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grey's Anatomy</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>256,067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Flash</td>\n",
       "      <td>(2014– )</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arrow</td>\n",
       "      <td>(2012–2020)</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>216,277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Money Heist</td>\n",
       "      <td>(2017– )</td>\n",
       "      <td>Action, Crime, Mystery</td>\n",
       "      <td>70 min</td>\n",
       "      <td>8.3</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Big Bang Theory</td>\n",
       "      <td>(2007–2019)</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>22 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>278,832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Black Mirror</td>\n",
       "      <td>(2011– )</td>\n",
       "      <td>Drama, Sci-Fi, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>8.8</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sherlock</td>\n",
       "      <td>(2010–2017)</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>88 min</td>\n",
       "      <td>9.1</td>\n",
       "      <td>117,803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vikings</td>\n",
       "      <td>(2013–2020)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.5</td>\n",
       "      <td>|</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name         Year                     Genre    time  \\\n",
       "0           Game of Thrones  (2011–2019)  Action, Adventure, Drama  57 min   \n",
       "1           Stranger Things     (2016– )    Drama, Fantasy, Horror  51 min   \n",
       "2          The Walking Dead     (2010– )   Drama, Horror, Thriller  44 min   \n",
       "3            13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller  60 min   \n",
       "4                   The 100  (2014–2020)    Drama, Mystery, Sci-Fi  43 min   \n",
       "5   Orange Is the New Black  (2013–2019)      Comedy, Crime, Drama  59 min   \n",
       "6                 Riverdale     (2017– )     Crime, Drama, Mystery  45 min   \n",
       "7            Grey's Anatomy     (2005– )            Drama, Romance  41 min   \n",
       "8                 The Flash     (2014– )  Action, Adventure, Drama  43 min   \n",
       "9                     Arrow  (2012–2020)  Action, Adventure, Crime  42 min   \n",
       "10              Money Heist     (2017– )    Action, Crime, Mystery  70 min   \n",
       "11      The Big Bang Theory  (2007–2019)           Comedy, Romance  22 min   \n",
       "12             Black Mirror     (2011– )   Drama, Sci-Fi, Thriller  60 min   \n",
       "13                 Sherlock  (2010–2017)     Crime, Drama, Mystery  88 min   \n",
       "14                  Vikings  (2013–2020)  Action, Adventure, Drama  44 min   \n",
       "\n",
       "   Rating       vote  \n",
       "0     9.3          |  \n",
       "1     8.7  1,768,795  \n",
       "2     8.2          |  \n",
       "3     7.6    821,278  \n",
       "4     7.6          |  \n",
       "5     8.1    852,923  \n",
       "6     6.9          |  \n",
       "7     7.6    256,067  \n",
       "8     7.7          |  \n",
       "9     7.5    216,277  \n",
       "10    8.3          |  \n",
       "11    8.1    278,832  \n",
       "12    8.8          |  \n",
       "13    9.1    117,803  \n",
       "14    8.5          |  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://archive.ics.uci.edu/ml/index.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "show=driver.find_element_by_xpath(\"/html/body/table[1]/tbody/tr/td[2]/span[2]/a/font/b\")\n",
    "try:\n",
    "    show.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(show.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_name=[]\n",
    "Data_type=[]\n",
    "Task=[]\n",
    "Attribute_type=[]\n",
    "No_instances=[]\n",
    "No_attribute=[]\n",
    "Year=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abalone\n",
      "Adult\n",
      "Annealing\n",
      "Anonymous Microsoft Web Data\n",
      "Arrhythmia\n",
      "Artificial Characters\n",
      "Audiology (Original)\n",
      "Audiology (Standardized)\n",
      "Auto MPG\n",
      "Automobile\n",
      "Badges\n",
      "Balance Scale\n",
      "Balloons\n",
      "Breast Cancer\n",
      "Breast Cancer Wisconsin (Original)\n",
      "Breast Cancer Wisconsin (Prognostic)\n",
      "Breast Cancer Wisconsin (Diagnostic)\n",
      "Pittsburgh Bridges\n",
      "Car Evaluation\n",
      "Census Income\n",
      "Chess (King-Rook vs. King-Knight)\n",
      "Chess (King-Rook vs. King-Pawn)\n",
      "Chess (King-Rook vs. King)\n",
      "Chess (Domain Theories)\n",
      "Bach Chorales\n",
      "Connect-4\n",
      "Credit Approval\n",
      "Japanese Credit Screening\n",
      "Computer Hardware\n",
      "Contraceptive Method Choice\n",
      "Covertype\n",
      "Cylinder Bands\n",
      "Dermatology\n",
      "Diabetes\n",
      "DGP2 - The Second Data Generation Program\n",
      "Document Understanding\n",
      "EBL Domain Theories\n",
      "Echocardiogram\n",
      "Ecoli\n",
      "Flags\n",
      "Function Finding\n",
      "Glass Identification\n",
      "Haberman's Survival\n",
      "Hayes-Roth\n",
      "Heart Disease\n",
      "Hepatitis\n",
      "Horse Colic\n",
      "ICU\n",
      "Image Segmentation\n",
      "Internet Advertisements\n",
      "Ionosphere\n",
      "Iris\n",
      "ISOLET\n",
      "Kinship\n",
      "Labor Relations\n",
      "LED Display Domain\n",
      "Lenses\n",
      "Letter Recognition\n",
      "Liver Disorders\n",
      "Logic Theorist\n",
      "Lung Cancer\n",
      "Lymphography\n",
      "Mechanical Analysis\n",
      "Meta-data\n",
      "Mobile Robots\n",
      "Molecular Biology (Promoter Gene Sequences)\n",
      "Molecular Biology (Protein Secondary Structure)\n",
      "Molecular Biology (Splice-junction Gene Sequences)\n",
      "MONK's Problems\n",
      "Moral Reasoner\n",
      "Multiple Features\n",
      "Mushroom\n",
      "Musk (Version 1)\n",
      "Musk (Version 2)\n",
      "Nursery\n",
      "Othello Domain Theory\n",
      "Page Blocks Classification\n",
      "Optical Recognition of Handwritten Digits\n",
      "Pen-Based Recognition of Handwritten Digits\n",
      "Post-Operative Patient\n",
      "Primary Tumor\n",
      "Prodigy\n",
      "Qualitative Structure Activity Relationships\n",
      "Quadruped Mammals\n",
      "Servo\n",
      "Shuttle Landing Control\n",
      "Solar Flare\n",
      "Soybean (Large)\n",
      "Soybean (Small)\n",
      "Challenger USA Space Shuttle O-Ring\n",
      "Low Resolution Spectrometer\n",
      "Spambase\n",
      "SPECT Heart\n",
      "SPECTF Heart\n",
      "Sponge\n",
      "Statlog Project\n",
      "Student Loan Relational\n",
      "Teaching Assistant Evaluation\n",
      "Tic-Tac-Toe Endgame\n",
      "Thyroid Disease\n",
      "Trains\n",
      "University\n",
      "Congressional Voting Records\n",
      "Water Treatment Plant\n",
      "Waveform Database Generator (Version 1)\n",
      "Waveform Database Generator (Version 2)\n",
      "Wine\n",
      "Yeast\n",
      "Zoo\n",
      "Undocumented\n",
      "Twenty Newsgroups\n",
      "Australian Sign Language signs\n",
      "Australian Sign Language signs (High Quality)\n",
      "US Census Data (1990)\n",
      "Census-Income (KDD)\n",
      "Coil 1999 Competition Data\n",
      "Corel Image Features\n",
      "E. Coli Genes\n",
      "EEG Database\n",
      "El Nino\n",
      "Entree Chicago Recommendation Data\n",
      "CMU Face Images\n",
      "Insurance Company Benchmark (COIL 2000)\n",
      "Internet Usage Data\n",
      "IPUMS Census Database\n",
      "Japanese Vowels\n",
      "KDD Cup 1998 Data\n",
      "KDD Cup 1999 Data\n",
      "M. Tuberculosis Genes\n",
      "Movie\n",
      "MSNBC.com Anonymous Web Data\n",
      "NSF Research Award Abstracts 1990-2003\n",
      "Pioneer-1 Mobile Robot Data\n",
      "Pseudo Periodic Synthetic Time Series\n",
      "Reuters-21578 Text Categorization Collection\n",
      "Robot Execution Failures\n",
      "Synthetic Control Chart Time Series\n",
      "Syskill and Webert Web Page Ratings\n",
      "UNIX User Data\n",
      "Volcanoes on Venus - JARtool experiment\n",
      "Statlog (Australian Credit Approval)\n",
      "Statlog (German Credit Data)\n",
      "Statlog (Heart)\n",
      "Statlog (Landsat Satellite)\n",
      "Statlog (Image Segmentation)\n",
      "Statlog (Shuttle)\n",
      "Statlog (Vehicle Silhouettes)\n",
      "Connectionist Bench (Nettalk Corpus)\n",
      "Connectionist Bench (Sonar, Mines vs. Rocks)\n",
      "Connectionist Bench (Vowel Recognition - Deterding Data)\n",
      "Economic Sanctions\n",
      "Protein Data\n",
      "Cloud\n",
      "CalIt2 Building People Counts\n",
      "Dodgers Loop Sensor\n",
      "Poker Hand\n",
      "MAGIC Gamma Telescope\n",
      "UJI Pen Characters\n",
      "Mammographic Mass\n",
      "Forest Fires\n",
      "Reuters Transcribed Subset\n",
      "Bag of Words\n",
      "Concrete Compressive Strength\n",
      "Hill-Valley\n",
      "Arcene\n",
      "Dexter\n",
      "Dorothea\n",
      "Gisette\n",
      "Madelon\n",
      "Ozone Level Detection\n",
      "Abscisic Acid Signaling Network\n",
      "Parkinsons\n",
      "Character Trajectories\n",
      "Blood Transfusion Service Center\n",
      "UJI Pen Characters (Version 2)\n",
      "Semeion Handwritten Digit\n",
      "SECOM\n",
      "Plants\n",
      "Libras Movement\n",
      "Concrete Slump Test\n",
      "Communities and Crime\n",
      "Acute Inflammations\n",
      "Wine Quality\n",
      "URL Reputation\n",
      "p53 Mutants\n",
      "Parkinsons Telemonitoring\n",
      "Demospongiae\n",
      "Opinosis Opinion ⁄ Review\n",
      "Breast Tissue\n",
      "Cardiotocography\n",
      "Wall-Following Robot Navigation Data\n",
      "Spoken Arabic Digit\n",
      "Localization Data for Person Activity\n",
      "AutoUniv\n",
      "Steel Plates Faults\n",
      "MiniBooNE particle identification\n",
      "YearPredictionMSD\n",
      "PEMS-SF\n",
      "OpinRank Review Dataset\n",
      "Relative location of CT slices on axial axis\n",
      "Online Handwritten Assamese Characters Dataset\n",
      "PubChem Bioassay Data\n",
      "Record Linkage Comparison Patterns\n",
      "Communities and Crime Unnormalized\n",
      "Vertebral Column\n",
      "EMG Physical Action Data Set\n",
      "Vicon Physical Action Data Set\n",
      "Amazon Commerce reviews set\n",
      "Amazon Access Samples\n",
      "Reuter_50_50\n",
      "Farm Ads\n",
      "DBWorld e-mails\n",
      "KEGG Metabolic Relation Network (Directed)\n",
      "KEGG Metabolic Reaction Network (Undirected)\n",
      "Bank Marketing\n",
      "YouTube Comedy Slam Preference Data\n",
      "Gas Sensor Array Drift Dataset\n",
      "ILPD (Indian Liver Patient Dataset)\n",
      "OPPORTUNITY Activity Recognition\n",
      "Nomao\n",
      "SMS Spam Collection\n",
      "Skin Segmentation\n",
      "Planning Relax\n",
      "PAMAP2 Physical Activity Monitoring\n",
      "Restaurant & consumer data\n",
      "CNAE-9\n",
      "Individual household electric power consumption\n",
      "seeds\n",
      "Northix\n",
      "QtyT40I10D100K\n",
      "Legal Case Reports\n",
      "Human Activity Recognition Using Smartphones\n",
      "One-hundred plant species leaves data set\n",
      "Energy efficiency\n",
      "Yacht Hydrodynamics\n",
      "Fertility\n",
      "Daphnet Freezing of Gait\n",
      "3D Road Network (North Jutland, Denmark)\n",
      "ISTANBUL STOCK EXCHANGE\n",
      "Buzz in social media\n",
      "First-order theorem proving\n",
      "Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)\n",
      "Gas sensor arrays in open sampling settings\n",
      "Climate Model Simulation Crashes\n",
      "MicroMass\n",
      "QSAR biodegradation\n",
      "BLOGGER\n",
      "Daily and Sports Activities\n",
      "User Knowledge Modeling\n",
      "Reuters RCV1 RCV2 Multilingual, Multiview Text Categorization Test collection\n",
      "NYSK\n",
      "Turkiye Student Evaluation\n",
      "ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\n",
      "EEG Eye State\n",
      "Physicochemical Properties of Protein Tertiary Structure\n",
      "seismic-bumps\n",
      "banknote authentication\n",
      "USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat\n",
      "YouTube Multiview Video Games Dataset\n",
      "Gas Sensor Array Drift Dataset at Different Concentrations\n",
      "Activities of Daily Living (ADLs) Recognition Using Binary Sensors\n",
      "SkillCraft1 Master Table Dataset\n",
      "Weight Lifting Exercises monitored with Inertial Measurement Units\n",
      "SML2010\n",
      "Bike Sharing Dataset\n",
      "Predict keywords activities in a online social media\n",
      "Thoracic Surgery Data\n",
      "EMG dataset in Lower Limb\n",
      "SUSY\n",
      "HIGGS\n",
      "Qualitative_Bankruptcy\n",
      "LSVT Voice Rehabilitation\n",
      "Dataset for ADL Recognition with Wrist-worn Accelerometer\n",
      "Wilt\n",
      "User Identification From Walking Activity\n",
      "Activity Recognition from Single Chest-Mounted Accelerometer\n",
      "Leaf\n",
      "Dresses_Attribute_Sales\n",
      "Tamilnadu Electricity Board Hourly Readings\n",
      "Airfoil Self-Noise\n",
      "Wholesale customers\n",
      "Twitter Data set for Arabic Sentiment Analysis\n",
      "Combined Cycle Power Plant\n",
      "Urban Land Cover\n",
      "Diabetes 130-US hospitals for years 1999-2008\n",
      "Bach Choral Harmony\n",
      "StoneFlakes\n",
      "Tennis Major Tournament Match Statistics\n",
      "Parkinson Speech Dataset with Multiple Types of Sound Recordings\n",
      "Gesture Phase Segmentation\n",
      "Perfume Data\n",
      "BlogFeedback\n",
      "REALDISP Activity Recognition Dataset\n",
      "Newspaper and magazine images segmentation dataset\n",
      "AAAI 2014 Accepted Papers\n",
      "Gas sensor array under flow modulation\n",
      "Gas sensor array exposed to turbulent gas mixtures\n",
      "UJIIndoorLoc\n",
      "Sentence Classification\n",
      "Dow Jones Index\n",
      "sEMG for Basic Hand movements\n",
      "AAAI 2013 Accepted Papers\n",
      "Geographical Original of Music\n",
      "Condition Based Maintenance of Naval Propulsion Plants\n",
      "Grammatical Facial Expressions\n",
      "NoisyOffice\n",
      "MHEALTH Dataset\n",
      "Student Performance\n",
      "ElectricityLoadDiagrams20112014\n",
      "Gas sensor array under dynamic gas mixtures\n",
      "microblogPCU\n",
      "Firm-Teacher_Clave-Direction_Classification\n",
      "Dataset for Sensorless Drive Diagnosis\n",
      "TV News Channel Commercial Detection Dataset\n",
      "Phishing Websites\n",
      "Greenhouse Gas Observing Network\n",
      "Diabetic Retinopathy Debrecen Data Set\n",
      "HIV-1 protease cleavage\n",
      "Sentiment Labelled Sentences\n",
      "Online News Popularity\n",
      "Forest type mapping\n",
      "wiki4HE\n",
      "Online Video Characteristics and Transcoding Time Dataset\n",
      "Chronic_Kidney_Disease\n",
      "Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014\n",
      "Folio\n",
      "Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015\n",
      "Cuff-Less Blood Pressure Estimation\n",
      "Smartphone-Based Recognition of Human Activities and Postural Transitions\n",
      "Mice Protein Expression\n",
      "UJIIndoorLoc-Mag\n",
      "Heterogeneity Activity Recognition\n",
      "Educational Process Mining (EPM): A Learning Analytics Data Set\n",
      "HEPMASS\n",
      "Indoor User Movement Prediction from RSS data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open University Learning Analytics dataset\n",
      "default of credit card clients\n",
      "Mesothelioma’s disease data set\n",
      "Online Retail\n",
      "SIFT10M\n",
      "GPS Trajectories\n",
      "Detect Malacious Executable(AntiVirus)\n",
      "Occupancy Detection\n",
      "Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson’s Disease\n",
      "News Aggregator\n",
      "Air Quality\n",
      "Twin gas sensor arrays\n",
      "Gas sensors for home activity monitoring\n",
      "Facebook Comment Volume Dataset\n",
      "Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL)\n",
      "Polish companies bankruptcy data\n",
      "Activity Recognition system based on Multisensor data fusion (AReM)\n",
      "Dota2 Games Results\n",
      "Facebook metrics\n",
      "UbiqLog (smartphone lifelogging)\n",
      "NIPS Conference Papers 1987-2015\n",
      "HTRU2\n",
      "Drug consumption (quantified)\n",
      "Appliances energy prediction\n",
      "Miskolc IIS Hybrid IPS\n",
      "KDC-4007 dataset Collection\n",
      "Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone\n",
      "DrivFace\n",
      "Website Phishing\n",
      "YouTube Spam Collection\n",
      "Beijing PM2.5 Data\n",
      "Cargo 2000 Freight Tracking and Tracing\n",
      "Cervical cancer (Risk Factors)\n",
      "Quality Assessment of Digital Colposcopies\n",
      "KASANDR\n",
      "FMA: A Dataset For Music Analysis\n",
      "Air quality\n",
      "Epileptic Seizure Recognition\n",
      "Devanagari Handwritten Character Dataset\n",
      "Stock portfolio performance\n",
      "MoCap Hand Postures\n",
      "Early biomarkers of Parkinson�s disease based on natural connected speech\n",
      "Data for Software Engineering Teamwork Assessment in Education Setting\n",
      "PM2.5 Data of Five Chinese Cities\n",
      "Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet\n",
      "Sales_Transactions_Dataset_Weekly\n",
      "Las Vegas Strip\n",
      "Eco-hotel\n",
      "MEU-Mobile KSD\n",
      "Crowdsourced Mapping\n",
      "gene expression cancer RNA-Seq\n",
      "Hybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer\n",
      "chestnut – LARVIC\n",
      "Burst Header Packet (BHP) flooding attack on Optical Burst Switching (OBS) Network\n",
      "Motion Capture Hand Postures\n",
      "Anuran Calls (MFCCs)\n",
      "TTC-3600: Benchmark dataset for Turkish text categorization\n",
      "Gastrointestinal Lesions in Regular Colonoscopy\n",
      "Daily Demand Forecasting Orders\n",
      "Paper Reviews\n",
      "extention of Z-Alizadeh sani dataset\n",
      "Z-Alizadeh Sani\n",
      "Dynamic Features of VirusShare Executables\n",
      "IDA2016Challenge\n",
      "DSRC Vehicle Communications\n",
      "Mturk User-Perceived Clusters over Images\n",
      "Character Font Images\n",
      "DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels\n",
      "Autistic Spectrum Disorder Screening Data for Children\n",
      "Autistic Spectrum Disorder Screening Data for Adolescent\n",
      "APS Failure at Scania Trucks\n",
      "Wireless Indoor Localization\n",
      "HCC Survival\n",
      "CSM (Conventional and Social Media Movies) Dataset 2014 and 2015\n",
      "University of Tehran Question Dataset 2016 (UTQD.2016)\n",
      "Autism Screening Adult\n",
      "Activity recognition with healthy older people using a batteryless wearable sensor\n",
      "Immunotherapy Dataset\n",
      "Cryotherapy Dataset\n",
      "OCT data & Color Fundus Images of Left & Right Eyes\n",
      "Discrete Tone Image Dataset\n",
      "News Popularity in Multiple Social Media Platforms\n",
      "Ultrasonic flowmeter diagnostics\n",
      "ICMLA 2014 Accepted Papers Data Set\n",
      "BLE RSSI Dataset for Indoor localization and Navigation\n",
      "Container Crane Controller Data Set\n",
      "Residential Building Data Set\n",
      "Health News in Twitter\n",
      "chipseq\n",
      "SGEMM GPU kernel performance\n",
      "Repeat Consumption Matrices\n",
      "detection_of_IoT_botnet_attacks_N_BaIoT\n",
      "Absenteeism at work\n",
      "SCADI\n",
      "Condition monitoring of hydraulic systems\n",
      "Carbon Nanotubes\n",
      "Optical Interconnection Network\n",
      "Sports articles for objectivity analysis\n",
      "Breast Cancer Coimbra\n",
      "GNFUV Unmanned Surface Vehicles Sensor Data\n",
      "Dishonest Internet users Dataset\n",
      "Victorian Era Authorship Attribution\n",
      "Simulated Falls and Daily Living Activities Data Set\n",
      "Multimodal Damage Identification for Humanitarian Computing\n",
      "EEG Steady-State Visual Evoked Potential Signals\n",
      "Roman Urdu Data Set\n",
      "Avila\n",
      "PANDOR\n",
      "Drug Review Dataset (Druglib.com)\n",
      "Drug Review Dataset (Drugs.com)\n",
      "Physical Unclonable Functions\n",
      "Superconductivty Data\n",
      "WESAD (Wearable Stress and Affect Detection)\n",
      "GNFUV Unmanned Surface Vehicles Sensor Data Set 2\n",
      "Student Academics Performance\n",
      "Online Shoppers Purchasing Intention Dataset\n",
      "PMU-UD\n",
      "Parkinson's Disease Classification\n",
      "Electrical Grid Stability Simulated Data\n",
      "Caesarian Section Classification Dataset\n",
      "BAUM-1\n",
      "BAUM-2\n",
      "Audit Data\n",
      "BuddyMove Data Set\n",
      "Real estate valuation data set\n",
      "Early biomarkers of Parkinson’s disease based on natural connected speech Data Set\n",
      "Somerville Happiness Survey\n",
      "2.4 GHZ Indoor Channel Measurements\n",
      "EMG data for gestures\n",
      "Parking Birmingham\n",
      "Behavior of the urban traffic of the city of Sao Paulo in Brazil\n",
      "Travel Reviews\n",
      "Tarvel Review Ratings\n",
      "Rice Leaf Diseases\n",
      "Gas sensor array temperature modulation\n",
      "Facebook Live Sellers in Thailand\n",
      "Parkinson Dataset with replicated acoustic features\n",
      "Metro Interstate Traffic Volume\n",
      "Query Analytics Workloads Dataset\n",
      "Wave Energy Converters\n",
      "PPG-DaLiA\n",
      "Alcohol QCM Sensor Dataset\n",
      "Divorce Predictors data set\n",
      "Incident management process enriched event log\n",
      "Opinion Corpus for Lebanese Arabic Reviews (OCLAR)\n",
      "MEx\n",
      "Beijing Multi-Site Air-Quality Data\n",
      "Online Retail II\n",
      "Hepatitis C Virus (HCV) for Egyptian patients\n",
      "QSAR fish toxicity\n",
      "QSAR aquatic toxicity\n",
      "Human Activity Recognition from Continuous Ambient Sensor Data\n",
      "WISDM Smartphone and Smartwatch Activity and Biometrics Dataset\n",
      "QSAR oral toxicity\n",
      "QSAR androgen receptor\n",
      "QSAR Bioconcentration classes dataset\n",
      "QSAR fish bioconcentration factor (BCF)\n",
      "A study of Asian Religious and Biblical Texts\n",
      "Real-time Election Results: Portugal 2019\n",
      "Bias correction of numerical prediction model temperature forecast\n",
      "Bar Crawl: Detecting Heavy Drinking\n",
      "Kitsune Network Attack Dataset\n",
      "Shoulder Implant X-Ray Manufacturer Classification\n",
      "Speaker Accent Recognition\n",
      "Heart failure clinical records\n",
      "Deepfakes: Medical Image Tamper Detection\n",
      "selfBACK\n",
      "South German Credit\n",
      "Exasens\n",
      "Swarm Behaviour\n",
      "Crop mapping using fused optical-radar data set\n",
      "BitcoinHeistRansomwareAddressDataset\n",
      "Facebook Large Page-Page Network\n",
      "Amphibians\n",
      "Early stage diabetes risk prediction dataset.\n",
      "Turkish Spam V01\n",
      "Stock keeping units\n",
      "Demand Forecasting for a store\n",
      "Detect Malware Types\n",
      "Wave Energy Converters\n",
      "Youtube cookery channels viewers comments in Hinglish\n",
      "Pedestrian in Traffic Dataset\n",
      "Cervical Cancer Behavior Risk\n",
      "Sattriya_Dance_Single_Hand_Gestures Dataset\n",
      "Divorce Predictors data set\n",
      "3W dataset\n",
      "Malware static and dynamic features VxHeaven and Virus Total\n",
      "Internet Firewall Data\n",
      "User Profiling and Abusive Language Detection Dataset\n",
      "Estimation of obesity levels based on eating habits and physical condition\n",
      "Rice (Cammeo and Osmancik)\n",
      "Vehicle routing and scheduling problems\n",
      "Algerian Forest Fires Dataset\n",
      "Breath Metabolomics\n",
      "Horton General Hospital\n",
      "UrbanGB, urban road accidents coordinates labelled by the urban center\n",
      "Gas Turbine CO and NOx Emission Data Set\n",
      "Activity recognition using wearable physiological measurements\n",
      "clickstream data for online shopping\n",
      "CNNpred: CNN-based stock market prediction using a diverse set of variables\n",
      "Apartment for rent classified\n",
      ": Simulated Data set of Iraqi tourism places\n",
      "Nasarian CAD Dataset\n",
      "Monolithic Columns in Troad and Mysia Region\n",
      "Bar Crawl: Detecting Heavy Drinking\n",
      "Seoul Bike Sharing Demand\n",
      "Person Classification Gait Data\n",
      "Shill Bidding Dataset\n",
      "Iranian Churn Dataset\n",
      "Unmanned Aerial Vehicle (UAV) Intrusion Detection\n",
      "Bone marrow transplant: children\n",
      "Exasens\n",
      "COVID-19 Surveillance\n",
      "Refractive errors\n",
      "Shoulder Implant X-Ray Manufacturer Classification\n",
      "CLINC150\n",
      "HCV data\n",
      "Taiwanese Bankruptcy Prediction\n",
      "South German Credit (UPDATE)\n",
      "IIWA14-R820-Gazebo-Dataset-10Trajectories\n",
      "Guitar Chords finger positions\n",
      "Russian Corpus of Biographical Texts\n",
      "Codon usage\n",
      "Intelligent Media Accelerometer and Gyroscope (IM-AccGyro) Dataset\n"
     ]
    }
   ],
   "source": [
    "dn=driver.find_elements_by_xpath(\"//p[@class='normal']/b/a\")\n",
    "for i in dn:\n",
    "    print(i.text)\n",
    "    Dataset_name.append(i.text)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types\n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Univariate, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Data-Generator \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Univariate, Time-Series \n",
      "Multivariate, Spatial \n",
      "Multivariate \n",
      "Multivariate, Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Data-Generator \n",
      " \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Relational \n",
      "Multivariate \n",
      "Multivariate, Data-Generator \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Sequential, Domain-Theory \n",
      "Sequential \n",
      "Sequential, Domain-Theory \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Domain-Theory \n",
      "Multivariate, Data-Generator \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Domain-Theory \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Data-Generator \n",
      "Multivariate, Data-Generator \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Text \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Relational \n",
      "Multivariate, Time-Series \n",
      "Spatio-temporal \n",
      "Transactional, Sequential \n",
      "Image \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Relational \n",
      "Multivariate, Relational \n",
      "Sequential \n",
      "Text \n",
      "Multivariate, Time-Series \n",
      "Univariate, Time-Series \n",
      "Text \n",
      "Multivariate, Time-Series \n",
      "Time-Series \n",
      "Multivariate, Text \n",
      "Text, Sequential \n",
      "Image \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Domain-Theory \n",
      " \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Text \n",
      "Multivariate \n",
      "Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Time-Series \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate, Time-Series \n",
      "Univariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Text \n",
      "Domain-Theory \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Time-Series \n",
      "Time-Series \n",
      "Multivariate, Text, Domain-Theory \n",
      "Time-Series, Domain-Theory \n",
      "Multivariate, Text, Domain-Theory \n",
      "Text \n",
      "Text \n",
      "Multivariate, Univariate, Text \n",
      "Multivariate, Univariate, Text \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Univariate \n",
      "Multivariate, Text, Domain-Theory \n",
      "Univariate \n",
      "Univariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Univariate, Text \n",
      "Sequential \n",
      "Text \n",
      "Multivariate, Time-Series \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Sequential, Text \n",
      "Multivariate, Univariate, Time-Series \n",
      "Time-Series, Multivariate \n",
      "Multivariate \n",
      "Sequential \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Domain-Theory \n",
      "Multivariate, Text \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series, Text \n",
      "Univariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      " \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Univariate, Sequential, Time-Series \n",
      "Univariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Univariate, Domain-Theory \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      " \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Text \n",
      "Time-Series \n",
      "Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Univariate, Sequential, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series, Domain-Theory \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series, Domain-Theory \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Time-Series \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Text \n",
      "Multivariate, Text \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Sequential, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      " \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      " \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Time-Series \n",
      "Text \n",
      " \n",
      " \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Sequential, Text \n",
      "Multivariate, Text \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      " \n",
      "Sequential \n",
      "Univariate \n",
      "Univariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Univariate, Domain-Theory \n",
      "Multivariate \n",
      "Text \n",
      "Sequential \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Univariate \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Text \n",
      "Time-Series \n",
      "Multivariate, Text \n",
      "Multivariate, Time-Series \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Univariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Univariate \n",
      "Time-Series \n",
      "Time-Series \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Multivariate \n",
      "Time-Series \n",
      "Multivariate, Univariate, Sequential, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Text \n",
      "Multivariate, Text \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate, Univariate \n",
      "Multivariate, Sequential \n",
      "Text \n",
      "Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential, Time-Series, Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate, Time-Series, Text \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series, Text \n",
      "Multivariate \n",
      "Multivariate, Text \n",
      "Multivariate, Sequential, Time-Series \n",
      "Multivariate, Univariate \n",
      "Multivariate \n",
      "Multivariate, Univariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate, Time-Series \n",
      "Univariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Sequential \n",
      "Sequential, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate, Time-Series \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      "Text \n",
      "Multivariate \n",
      "Multivariate \n",
      "Multivariate \n",
      " \n",
      "Text \n",
      "Text \n",
      "Multivariate \n",
      "Time-Series \n"
     ]
    }
   ],
   "source": [
    "dt=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]/p\")\n",
    "for i in dt:\n",
    "    print(i.text)\n",
    "    Data_type.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Task\n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Recommender-Systems \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Function-Learning \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Relational-Learning \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Clustering \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Clustering \n",
      "Classification \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Recommender-Systems \n",
      "Classification \n",
      "Regression, Description \n",
      " \n",
      " \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Clustering \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Causal-Discovery \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Causal-Discovery \n",
      "Clustering \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Regression \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      " \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression, Clustering, Causal-Discovery \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Regression, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Regression, Clustering \n",
      "Classification, Regression \n",
      "Regression, Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Regression, Clustering, Causa \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification \n",
      "Regression \n",
      "Regression \n",
      " \n",
      "Classification \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Clustering, Causal-Discovery \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Clustering \n",
      "Classification, Regression \n",
      "Classification, Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Clustering \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification, Regression \n",
      "Regression, Clustering \n",
      "Classification, Regression \n",
      "Classification, Causal-Discovery \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Regression, Clustering, Causal-Discovery \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Clustering, Causal-Discovery \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Causal-Discovery \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Causal-Discovery \n",
      "Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification, Clustering, Causal-Discovery \n",
      "Classification, Regression \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Causal-Discovery \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Classification, Regression \n",
      "Classification \n",
      "Regression \n",
      "Classification, Regression, Clustering \n",
      "Clustering \n",
      "Classification, Regression \n",
      " \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Clustering \n",
      "Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification, Regression \n",
      "Regression \n",
      "Clustering \n",
      "Classification \n",
      "Regression \n",
      "Clustering \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Recommendation \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression \n",
      "Classification, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Regression \n",
      "Clustering \n",
      "Classification \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression \n",
      "Regression, Clustering \n",
      "Regression \n",
      "Regression \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Regression, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Regression \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification, Clustering \n",
      "Regression \n",
      "Regression \n",
      "Classification, Regression \n",
      "Classification, Clustering, Causal-Discovery \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Regression, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Clustering \n",
      "Regression \n",
      "Classification \n",
      "Regression \n",
      "Classification \n",
      "Classification, Regression, Causal-Discovery \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification \n",
      "Clustering \n",
      "Classification, Regression \n",
      "Classification, Clustering \n",
      "Causal-Discovery \n",
      "Clustering \n",
      "Regression, Clustering \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Classification, Regression \n",
      "Classification, Regression, Clustering \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification, Regression \n",
      "Regression \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification, Regression \n",
      "Classification \n",
      "Classification, Regression \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n",
      "Classification, Regression, Clustering \n",
      "Regression \n",
      "Classification \n",
      "Classification \n",
      "Classification, Clustering \n",
      "Classification \n"
     ]
    }
   ],
   "source": [
    "tt=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]/p\")\n",
    "for i in tt:\n",
    "    print(i.text)\n",
    "    Task.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Types\n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer \n",
      "Categorical, Integer, Real \n",
      "Categorical \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer, Real \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical, Real \n",
      "Categorical, Integer, Real \n",
      " \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Categorical, Integer \n",
      " \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Categorical, Integer, Real \n",
      "Categorical, Real, Integer \n",
      "Integer \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Real \n",
      " \n",
      " \n",
      "Categorical, Integer, Real \n",
      "Real \n",
      "Categorical, Integer \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Categorical \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer, Real \n",
      "Real \n",
      "Real \n",
      "Categorical, Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Categorical \n",
      "Categorical, Integer, Real \n",
      "Categorical \n",
      "Categorical \n",
      "Integer \n",
      "Categorical, Integer, Real \n",
      " \n",
      "Integer \n",
      "Categorical \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer, Real \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical \n",
      " \n",
      "Integer, Real \n",
      "Categorical \n",
      "Integer \n",
      "Integer \n",
      "Categorical \n",
      " \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer \n",
      "Categorical, Integer \n",
      "Categorical \n",
      " \n",
      " \n",
      "Real \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical \n",
      "Categorical \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Categorical \n",
      "Integer \n",
      "Categorical, Integer \n",
      " \n",
      " \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Categorical, Real \n",
      "Categorical \n",
      "Categorical, Integer \n",
      "Categorical \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Categorical, Integer \n",
      " \n",
      " \n",
      "Categorical, Real \n",
      "Real \n",
      "Categorical \n",
      "Categorical, Integer \n",
      "Categorical, Real \n",
      "Real \n",
      " \n",
      "Categorical, Integer, Real \n",
      "Integer, Real \n",
      "Categorical \n",
      "Integer \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Real \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      " \n",
      " \n",
      "Categorical \n",
      " \n",
      "Categorical, Real \n",
      " \n",
      "Categorical \n",
      "Integer \n",
      "Real \n",
      "Categorical \n",
      " \n",
      " \n",
      "Categorical, Integer, Real \n",
      "Categorical, Integer \n",
      "Categorical, Real \n",
      "Integer \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Categorical \n",
      "Real \n",
      "Real \n",
      " \n",
      " \n",
      "Real \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Categorical, Integer \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Categorical \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Categorical, Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Categorical, Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      " \n",
      " \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      " \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      " \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      " \n",
      " \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      " \n",
      " \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      " \n",
      "Real \n",
      " \n",
      "Integer \n",
      " \n",
      "Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Categorical \n",
      " \n",
      "Integer, Real \n",
      " \n",
      " \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Integer \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      " \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      "Integer, Real \n",
      " \n",
      "Real \n",
      "Real \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      " \n",
      " \n",
      "Integer \n",
      "Integer \n",
      "Integer \n",
      " \n",
      "Real \n",
      "Categorical \n",
      "Integer \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      " \n",
      "Integer, Real \n",
      " \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      " \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Integer \n",
      " \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Integer \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      " \n",
      " \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      " \n",
      "Integer, Real \n",
      " \n",
      " \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      "Real \n",
      " \n",
      "Real \n",
      "Integer \n",
      " \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer, Real \n",
      " \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      "Real \n",
      "Integer \n",
      "Real \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      " \n",
      " \n",
      "Real \n",
      "Real \n",
      "Integer, Real \n",
      "Real \n",
      " \n",
      "Integer \n",
      "Real \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      "Integer \n",
      "Real \n",
      " \n",
      "Integer, Real \n",
      "Integer \n",
      "Integer, Real \n",
      "Integer \n",
      " \n",
      " \n",
      " \n",
      "Real \n"
     ]
    }
   ],
   "source": [
    "at=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]/p\")\n",
    "for i in at:\n",
    "    print(i.text)\n",
    "    Attribute_type.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Instances\n",
      "4177 \n",
      "48842 \n",
      "798 \n",
      "37711 \n",
      "452 \n",
      "6000 \n",
      "226 \n",
      "226 \n",
      "398 \n",
      "205 \n",
      "294 \n",
      "625 \n",
      "16 \n",
      "286 \n",
      "699 \n",
      "198 \n",
      "569 \n",
      "108 \n",
      "1728 \n",
      "48842 \n",
      " \n",
      "3196 \n",
      "28056 \n",
      " \n",
      "100 \n",
      "67557 \n",
      "690 \n",
      "125 \n",
      "209 \n",
      "1473 \n",
      "581012 \n",
      "512 \n",
      "366 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "132 \n",
      "336 \n",
      "194 \n",
      "352 \n",
      "214 \n",
      "306 \n",
      "160 \n",
      "303 \n",
      "155 \n",
      "368 \n",
      " \n",
      "2310 \n",
      "3279 \n",
      "351 \n",
      "150 \n",
      "7797 \n",
      "104 \n",
      "57 \n",
      " \n",
      "24 \n",
      "20000 \n",
      "345 \n",
      " \n",
      "32 \n",
      "148 \n",
      "209 \n",
      "528 \n",
      " \n",
      "106 \n",
      "128 \n",
      "3190 \n",
      "432 \n",
      "202 \n",
      "2000 \n",
      "8124 \n",
      "476 \n",
      "6598 \n",
      "12960 \n",
      " \n",
      "5473 \n",
      "5620 \n",
      "10992 \n",
      "90 \n",
      "339 \n",
      " \n",
      " \n",
      " \n",
      "167 \n",
      "15 \n",
      "1389 \n",
      "307 \n",
      "47 \n",
      "23 \n",
      "531 \n",
      "4601 \n",
      "267 \n",
      "267 \n",
      "76 \n",
      " \n",
      "1000 \n",
      "151 \n",
      "958 \n",
      "7200 \n",
      "10 \n",
      "285 \n",
      "435 \n",
      "527 \n",
      "5000 \n",
      "5000 \n",
      "178 \n",
      "1484 \n",
      "101 \n",
      " \n",
      "20000 \n",
      "6650 \n",
      "2565 \n",
      "2458285 \n",
      "299285 \n",
      "340 \n",
      "68040 \n",
      " \n",
      "122 \n",
      "178080 \n",
      "50672 \n",
      "640 \n",
      "9000 \n",
      "10104 \n",
      "256932 \n",
      "640 \n",
      "191779 \n",
      "4000000 \n",
      " \n",
      "10000 \n",
      "989818 \n",
      "129000 \n",
      " \n",
      "100000 \n",
      "21578 \n",
      "463 \n",
      "600 \n",
      "332 \n",
      " \n",
      " \n",
      "690 \n",
      "1000 \n",
      "270 \n",
      "6435 \n",
      "2310 \n",
      "58000 \n",
      "946 \n",
      "20008 \n",
      "208 \n",
      "528 \n",
      " \n",
      " \n",
      "1024 \n",
      "10080 \n",
      "50400 \n",
      "1025010 \n",
      "19020 \n",
      "1364 \n",
      "961 \n",
      "517 \n",
      "200 \n",
      "8000000 \n",
      "1030 \n",
      "606 \n",
      "900 \n",
      "2600 \n",
      "1950 \n",
      "13500 \n",
      "4400 \n",
      "2536 \n",
      "300 \n",
      "197 \n",
      "2858 \n",
      "748 \n",
      "11640 \n",
      "1593 \n",
      "1567 \n",
      "22632 \n",
      "360 \n",
      "103 \n",
      "1994 \n",
      "120 \n",
      "4898 \n",
      "2396130 \n",
      "16772 \n",
      "5875 \n",
      "503 \n",
      "51 \n",
      "106 \n",
      "2126 \n",
      "5456 \n",
      "8800 \n",
      "164860 \n",
      " \n",
      "1941 \n",
      "130065 \n",
      "515345 \n",
      "440 \n",
      " \n",
      "53500 \n",
      "8235 \n",
      " \n",
      "5749132 \n",
      "2215 \n",
      "310 \n",
      "10000 \n",
      "3000 \n",
      "1500 \n",
      "30000 \n",
      "2500 \n",
      "4143 \n",
      "64 \n",
      "53414 \n",
      "65554 \n",
      "45211 \n",
      "1138562 \n",
      "13910 \n",
      "583 \n",
      "2551 \n",
      "34465 \n",
      "5574 \n",
      "245057 \n",
      "182 \n",
      "3850505 \n",
      "138 \n",
      "1080 \n",
      "2075259 \n",
      "210 \n",
      "115 \n",
      "3960456 \n",
      " \n",
      "10299 \n",
      "1600 \n",
      "768 \n",
      "308 \n",
      "100 \n",
      "237 \n",
      "434874 \n",
      "536 \n",
      "140000 \n",
      "6118 \n",
      "165632 \n",
      "18000 \n",
      "540 \n",
      "931 \n",
      "1055 \n",
      "100 \n",
      "9120 \n",
      "403 \n",
      "111740 \n",
      "10421 \n",
      "5820 \n",
      "403 \n",
      "14980 \n",
      "45730 \n",
      "2584 \n",
      "1372 \n",
      "306 \n",
      "120000 \n",
      "13910 \n",
      "2747 \n",
      "3395 \n",
      "39242 \n",
      "4137 \n",
      "17389 \n",
      "51 \n",
      "470 \n",
      "132 \n",
      "5000000 \n",
      "11000000 \n",
      "250 \n",
      "126 \n",
      " \n",
      "4889 \n",
      " \n",
      " \n",
      "340 \n",
      "501 \n",
      "45781 \n",
      "1503 \n",
      "440 \n",
      "2000 \n",
      "9568 \n",
      "168 \n",
      "100000 \n",
      "5665 \n",
      "79 \n",
      "127 \n",
      "1040 \n",
      "9900 \n",
      "560 \n",
      "60021 \n",
      "1419 \n",
      "101 \n",
      "399 \n",
      "58 \n",
      "180 \n",
      "21048 \n",
      " \n",
      "750 \n",
      "3000 \n",
      "150 \n",
      "1059 \n",
      "11934 \n",
      "27965 \n",
      "216 \n",
      "120 \n",
      "649 \n",
      "370 \n",
      "4178504 \n",
      "221579 \n",
      "10800 \n",
      "58509 \n",
      "129685 \n",
      "2456 \n",
      "2921 \n",
      "1151 \n",
      "6590 \n",
      "3000 \n",
      "39797 \n",
      "326 \n",
      "913 \n",
      "168286 \n",
      "400 \n",
      "314080 \n",
      "637 \n",
      "1710671 \n",
      "12000 \n",
      "10929 \n",
      "1080 \n",
      "40000 \n",
      "43930257 \n",
      "230318 \n",
      "10500000 \n",
      "13197 \n",
      " \n",
      "30000 \n",
      "324 \n",
      "541909 \n",
      "11164866 \n",
      "163 \n",
      "373 \n",
      "20560 \n",
      "40 \n",
      "422937 \n",
      "9358 \n",
      "640 \n",
      "919438 \n",
      "40949 \n",
      "5744 \n",
      "10503 \n",
      "42240 \n",
      "102944 \n",
      "500 \n",
      "9782222 \n",
      "11463 \n",
      "17898 \n",
      "1885 \n",
      "19735 \n",
      "1540 \n",
      "4007 \n",
      "153540 \n",
      "606 \n",
      "1353 \n",
      "1956 \n",
      "43824 \n",
      "3942 \n",
      "858 \n",
      "287 \n",
      "17764280 \n",
      "106574 \n",
      "9358 \n",
      "11500 \n",
      "92000 \n",
      "315 \n",
      "78095 \n",
      "130 \n",
      "74 \n",
      "52854 \n",
      "77 \n",
      "811 \n",
      "504 \n",
      "401 \n",
      "2856 \n",
      "10546 \n",
      "801 \n",
      "1540 \n",
      "1451 \n",
      "1075 \n",
      "78095 \n",
      "7195 \n",
      "3600 \n",
      "76 \n",
      "60 \n",
      "405 \n",
      "303 \n",
      "303 \n",
      "107888 \n",
      "76000 \n",
      "10000 \n",
      "180 \n",
      "745000 \n",
      "12234 \n",
      "292 \n",
      "104 \n",
      "60000 \n",
      "2000 \n",
      "165 \n",
      "217 \n",
      "1175 \n",
      "704 \n",
      "75128 \n",
      "90 \n",
      "90 \n",
      "50 \n",
      "71 \n",
      "93239 \n",
      "540 \n",
      "105 \n",
      "6611 \n",
      "15 \n",
      "372 \n",
      "58000 \n",
      "4960 \n",
      "241600 \n",
      "130000 \n",
      "7062606 \n",
      "740 \n",
      "70 \n",
      "2205 \n",
      "10721 \n",
      "640 \n",
      "1000 \n",
      "116 \n",
      "1672 \n",
      "322 \n",
      "93600 \n",
      "3060 \n",
      "5879 \n",
      "9200 \n",
      "20000 \n",
      "20867 \n",
      " \n",
      "4143 \n",
      "215063 \n",
      "6000000 \n",
      "21263 \n",
      "63000000 \n",
      "10190 \n",
      "300 \n",
      "12330 \n",
      "5180 \n",
      "756 \n",
      "10000 \n",
      "80 \n",
      "1184 \n",
      "1047 \n",
      "777 \n",
      "249 \n",
      "414 \n",
      " \n",
      "143 \n",
      "7840 \n",
      "30000 \n",
      "35717 \n",
      "135 \n",
      "980 \n",
      "5456 \n",
      "120 \n",
      "4095000 \n",
      "7051 \n",
      "240 \n",
      "48204 \n",
      "260000 \n",
      "288000 \n",
      "8300000 \n",
      "125 \n",
      "170 \n",
      "141712 \n",
      "3916 \n",
      "6262 \n",
      "420768 \n",
      "1067371 \n",
      "1385 \n",
      "908 \n",
      "546 \n",
      "13956534 \n",
      "15630426 \n",
      "8992 \n",
      "1687 \n",
      "779 \n",
      "1056 \n",
      "590 \n",
      "21643 \n",
      "7750 \n",
      "14057567 \n",
      "27170754 \n",
      "597 \n",
      "329 \n",
      "299 \n",
      "20000 \n",
      "26136 \n",
      "1000 \n",
      "399 \n",
      "24017 \n",
      "325834 \n",
      "2916697 \n",
      "22470 \n",
      "189 \n",
      "520 \n",
      "826 \n",
      "2279 \n",
      "28764 \n",
      "7107 \n",
      "288000 \n",
      "9800 \n",
      "4760 \n",
      "72 \n",
      "1450 \n",
      "170 \n",
      "1984 \n",
      "2955 \n",
      "65532 \n",
      "65919 \n",
      "2111 \n",
      "3810 \n",
      "18 \n",
      "244 \n",
      "104 \n",
      "139 \n",
      "360177 \n",
      "36733 \n",
      "4480 \n",
      "165474 \n",
      "1985 \n",
      "10000 \n",
      "232 \n",
      "150 \n",
      "11 \n",
      "14057567 \n",
      "8760 \n",
      "48 \n",
      "6321 \n",
      "3150 \n",
      "17256 \n",
      "187 \n",
      "399 \n",
      "14 \n",
      "467 \n",
      "597 \n",
      "23700 \n",
      "615 \n",
      "6819 \n",
      "1000 \n",
      " \n",
      "2633 \n",
      "200 \n",
      "13028 \n",
      "800 \n"
     ]
    }
   ],
   "source": [
    "ni=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]/p\")\n",
    "for i in ni:\n",
    "    print(i.text)\n",
    "    No_instances.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Attributes\n",
      "8 \n",
      "14 \n",
      "38 \n",
      "294 \n",
      "279 \n",
      "7 \n",
      " \n",
      "69 \n",
      "8 \n",
      "26 \n",
      "1 \n",
      "4 \n",
      "4 \n",
      "9 \n",
      "10 \n",
      "34 \n",
      "32 \n",
      "13 \n",
      "6 \n",
      "14 \n",
      "22 \n",
      "36 \n",
      "6 \n",
      " \n",
      "6 \n",
      "42 \n",
      "15 \n",
      " \n",
      "9 \n",
      "9 \n",
      "54 \n",
      "39 \n",
      "33 \n",
      "20 \n",
      " \n",
      " \n",
      " \n",
      "12 \n",
      "8 \n",
      "30 \n",
      " \n",
      "10 \n",
      "3 \n",
      "5 \n",
      "75 \n",
      "19 \n",
      "27 \n",
      " \n",
      "19 \n",
      "1558 \n",
      "34 \n",
      "4 \n",
      "617 \n",
      "12 \n",
      "16 \n",
      "7 \n",
      "4 \n",
      "16 \n",
      "7 \n",
      " \n",
      "56 \n",
      "18 \n",
      "8 \n",
      "22 \n",
      " \n",
      "58 \n",
      " \n",
      "61 \n",
      "7 \n",
      " \n",
      "649 \n",
      "22 \n",
      "168 \n",
      "168 \n",
      "8 \n",
      " \n",
      "10 \n",
      "64 \n",
      "16 \n",
      "8 \n",
      "17 \n",
      " \n",
      " \n",
      "72 \n",
      "4 \n",
      "6 \n",
      "10 \n",
      "35 \n",
      "35 \n",
      "4 \n",
      "102 \n",
      "57 \n",
      "22 \n",
      "44 \n",
      "45 \n",
      " \n",
      " \n",
      "5 \n",
      "9 \n",
      "21 \n",
      "32 \n",
      "17 \n",
      "16 \n",
      "38 \n",
      "21 \n",
      "40 \n",
      "13 \n",
      "8 \n",
      "17 \n",
      " \n",
      " \n",
      "15 \n",
      "22 \n",
      "68 \n",
      "40 \n",
      "17 \n",
      "89 \n",
      " \n",
      "4 \n",
      "12 \n",
      " \n",
      " \n",
      "86 \n",
      "72 \n",
      "61 \n",
      "12 \n",
      "481 \n",
      "42 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "5 \n",
      "90 \n",
      " \n",
      "5 \n",
      " \n",
      " \n",
      "14 \n",
      "20 \n",
      "13 \n",
      "36 \n",
      "19 \n",
      "9 \n",
      "18 \n",
      "4 \n",
      "60 \n",
      "10 \n",
      " \n",
      " \n",
      "10 \n",
      "4 \n",
      "3 \n",
      "11 \n",
      "11 \n",
      " \n",
      "6 \n",
      "13 \n",
      " \n",
      "100000 \n",
      "9 \n",
      "101 \n",
      "10000 \n",
      "20000 \n",
      "100000 \n",
      "5000 \n",
      "500 \n",
      "73 \n",
      "43 \n",
      "23 \n",
      "3 \n",
      "5 \n",
      " \n",
      "256 \n",
      "591 \n",
      "70 \n",
      "91 \n",
      "10 \n",
      "128 \n",
      "6 \n",
      "12 \n",
      "3231961 \n",
      "5409 \n",
      "26 \n",
      " \n",
      " \n",
      "10 \n",
      "23 \n",
      "24 \n",
      "13 \n",
      "8 \n",
      " \n",
      "27 \n",
      "50 \n",
      "90 \n",
      "138672 \n",
      " \n",
      "386 \n",
      " \n",
      " \n",
      "12 \n",
      "147 \n",
      "6 \n",
      "8 \n",
      "27 \n",
      "10000 \n",
      "20000 \n",
      "10000 \n",
      "54877 \n",
      "4702 \n",
      "24 \n",
      "29 \n",
      "17 \n",
      "3 \n",
      "128 \n",
      "10 \n",
      "242 \n",
      "120 \n",
      " \n",
      "4 \n",
      "13 \n",
      "52 \n",
      "47 \n",
      "857 \n",
      "9 \n",
      "7 \n",
      "200 \n",
      "4 \n",
      " \n",
      "561 \n",
      "64 \n",
      "8 \n",
      "7 \n",
      "10 \n",
      "9 \n",
      "4 \n",
      "8 \n",
      "77 \n",
      "51 \n",
      "18 \n",
      "1950000 \n",
      "18 \n",
      "1300 \n",
      "41 \n",
      "6 \n",
      "5625 \n",
      "5 \n",
      " \n",
      "7 \n",
      "33 \n",
      "5 \n",
      "15 \n",
      "9 \n",
      "19 \n",
      "5 \n",
      "5 \n",
      "1000000 \n",
      "129 \n",
      " \n",
      "20 \n",
      "152 \n",
      "24 \n",
      "16 \n",
      "35 \n",
      "17 \n",
      "5 \n",
      "18 \n",
      "28 \n",
      "7 \n",
      "309 \n",
      "3 \n",
      "6 \n",
      " \n",
      " \n",
      "16 \n",
      "13 \n",
      "5 \n",
      "6 \n",
      "8 \n",
      "2 \n",
      "4 \n",
      "148 \n",
      "55 \n",
      "17 \n",
      "8 \n",
      "42 \n",
      "26 \n",
      "50 \n",
      "2 \n",
      "281 \n",
      "120 \n",
      " \n",
      "6 \n",
      "120432 \n",
      "150000 \n",
      "529 \n",
      " \n",
      "16 \n",
      "2500 \n",
      "5 \n",
      "68 \n",
      "16 \n",
      "100 \n",
      "216 \n",
      "23 \n",
      "33 \n",
      "140256 \n",
      "19 \n",
      "20 \n",
      "20 \n",
      "49 \n",
      "12 \n",
      "30 \n",
      "5232 \n",
      "20 \n",
      "1 \n",
      " \n",
      "61 \n",
      "27 \n",
      "53 \n",
      "11 \n",
      "25 \n",
      "0 \n",
      "20 \n",
      "9 \n",
      "3 \n",
      "561 \n",
      "82 \n",
      "13 \n",
      "16 \n",
      "13 \n",
      "28 \n",
      "4 \n",
      " \n",
      "24 \n",
      "34 \n",
      "8 \n",
      "128 \n",
      "15 \n",
      "513 \n",
      "7 \n",
      "7 \n",
      "5 \n",
      "15 \n",
      "480000 \n",
      "11 \n",
      "54 \n",
      "561 \n",
      "64 \n",
      "6 \n",
      "116 \n",
      "19 \n",
      " \n",
      "5812 \n",
      "9 \n",
      "32 \n",
      "29 \n",
      "67 \n",
      " \n",
      "25 \n",
      "6400 \n",
      "10 \n",
      "5 \n",
      "13 \n",
      "98 \n",
      "36 \n",
      "69 \n",
      "2158859 \n",
      "518 \n",
      "15 \n",
      "179 \n",
      " \n",
      "12 \n",
      "38 \n",
      "65 \n",
      "102 \n",
      "86 \n",
      "7 \n",
      "53 \n",
      "20 \n",
      "1 \n",
      "71 \n",
      "29 \n",
      "20531 \n",
      "65 \n",
      "3 \n",
      "22 \n",
      "38 \n",
      "22 \n",
      "4814 \n",
      "698 \n",
      "13 \n",
      "10 \n",
      "59 \n",
      "56 \n",
      "482 \n",
      "171 \n",
      "5 \n",
      "500 \n",
      "411 \n",
      "8519 \n",
      "21 \n",
      "21 \n",
      "171 \n",
      "7 \n",
      "49 \n",
      "12 \n",
      "3 \n",
      "21 \n",
      "9 \n",
      "8 \n",
      "7 \n",
      "2 \n",
      "11 \n",
      "11 \n",
      "173 \n",
      "5 \n",
      "15 \n",
      "3 \n",
      "105 \n",
      "25000 \n",
      " \n",
      "18 \n",
      "21000 \n",
      "115 \n",
      "21 \n",
      "206 \n",
      "43680 \n",
      "8 \n",
      "10 \n",
      "59 \n",
      "10 \n",
      "5 \n",
      "5 \n",
      "1000 \n",
      "138 \n",
      " \n",
      "16 \n",
      "2 \n",
      "10 \n",
      " \n",
      "8 \n",
      "6 \n",
      "129 \n",
      "81 \n",
      "12 \n",
      "6 \n",
      "22 \n",
      "18 \n",
      "9 \n",
      "754 \n",
      "14 \n",
      "5 \n",
      " \n",
      " \n",
      "18 \n",
      "7 \n",
      "7 \n",
      " \n",
      "7 \n",
      "5 \n",
      "6 \n",
      "4 \n",
      "18 \n",
      "11 \n",
      "25 \n",
      " \n",
      "20 \n",
      "12 \n",
      "46 \n",
      "9 \n",
      "8 \n",
      "49 \n",
      "11 \n",
      "8 \n",
      "54 \n",
      "36 \n",
      "3916 \n",
      "710 \n",
      "18 \n",
      "8 \n",
      "29 \n",
      "7 \n",
      "9 \n",
      "37 \n",
      "6 \n",
      "1024 \n",
      "1024 \n",
      "14 \n",
      "7 \n",
      "8265 \n",
      "29 \n",
      "25 \n",
      "3 \n",
      "115 \n",
      "1 \n",
      "12 \n",
      "13 \n",
      "200000 \n",
      "6 \n",
      "21 \n",
      "4 \n",
      "2400 \n",
      "175 \n",
      "10 \n",
      "4714 \n",
      "23 \n",
      "17 \n",
      "2 \n",
      "9 \n",
      "8 \n",
      "280 \n",
      "49 \n",
      "3 \n",
      "14 \n",
      "19 \n",
      " \n",
      "54 \n",
      "8 \n",
      "1087 \n",
      "12 \n",
      "3 \n",
      "17 \n",
      "8 \n",
      "9 \n",
      "12 \n",
      "1656 \n",
      "6 \n",
      "2 \n",
      "11 \n",
      "533 \n",
      "14 \n",
      "84 \n",
      "22 \n",
      "16 \n",
      "52 \n",
      "19 \n",
      "3 \n",
      "14 \n",
      "321 \n",
      "13 \n",
      "13 \n",
      "55 \n",
      "39 \n",
      "4 \n",
      "7 \n",
      "79 \n",
      "1 \n",
      " \n",
      "14 \n",
      "96 \n",
      "21 \n",
      " \n",
      "5 \n",
      "2 \n",
      "69 \n",
      "9 \n"
     ]
    }
   ],
   "source": [
    "na=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]/p\")\n",
    "for i in na:\n",
    "    print(i.text)\n",
    "    No_attribute.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year\n",
      "1995 \n",
      "1996 \n",
      " \n",
      "1998 \n",
      "1998 \n",
      "1992 \n",
      "1987 \n",
      "1992 \n",
      "1993 \n",
      "1987 \n",
      "1994 \n",
      "1994 \n",
      " \n",
      "1988 \n",
      "1992 \n",
      "1995 \n",
      "1995 \n",
      "1990 \n",
      "1997 \n",
      "1996 \n",
      "1988 \n",
      "1989 \n",
      "1994 \n",
      " \n",
      " \n",
      "1995 \n",
      " \n",
      "1992 \n",
      "1987 \n",
      "1997 \n",
      "1998 \n",
      "1995 \n",
      "1998 \n",
      " \n",
      " \n",
      "1994 \n",
      " \n",
      "1989 \n",
      "1996 \n",
      "1990 \n",
      "1990 \n",
      "1987 \n",
      "1999 \n",
      "1989 \n",
      "1988 \n",
      "1988 \n",
      "1989 \n",
      " \n",
      "1990 \n",
      "1998 \n",
      "1989 \n",
      "1988 \n",
      "1994 \n",
      "1990 \n",
      "1988 \n",
      "1988 \n",
      "1990 \n",
      "1991 \n",
      "1990 \n",
      " \n",
      "1992 \n",
      "1988 \n",
      "1990 \n",
      "1996 \n",
      "1995 \n",
      "1990 \n",
      " \n",
      "1992 \n",
      "1992 \n",
      "1994 \n",
      " \n",
      "1987 \n",
      "1994 \n",
      "1994 \n",
      "1997 \n",
      "1991 \n",
      "1995 \n",
      "1998 \n",
      "1998 \n",
      "1993 \n",
      "1988 \n",
      " \n",
      " \n",
      "1992 \n",
      "1993 \n",
      "1988 \n",
      "1989 \n",
      "1988 \n",
      "1987 \n",
      "1993 \n",
      "1988 \n",
      "1999 \n",
      "2001 \n",
      "2001 \n",
      " \n",
      "1992 \n",
      "1993 \n",
      "1997 \n",
      "1991 \n",
      "1987 \n",
      "1994 \n",
      "1988 \n",
      "1987 \n",
      "1993 \n",
      "1988 \n",
      "1988 \n",
      "1991 \n",
      "1996 \n",
      "1990 \n",
      " \n",
      "1999 \n",
      "1999 \n",
      "2002 \n",
      " \n",
      "2000 \n",
      "1999 \n",
      "1999 \n",
      "2001 \n",
      "1999 \n",
      "1999 \n",
      "2000 \n",
      "1999 \n",
      "2000 \n",
      "1999 \n",
      "1999 \n",
      " \n",
      "1998 \n",
      "1999 \n",
      "2001 \n",
      "1999 \n",
      " \n",
      "2003 \n",
      "1999 \n",
      "1999 \n",
      "1997 \n",
      "1999 \n",
      "1999 \n",
      "1998 \n",
      " \n",
      " \n",
      " \n",
      "1994 \n",
      " \n",
      "1993 \n",
      "1990 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "1989 \n",
      "2006 \n",
      "2006 \n",
      "2007 \n",
      "2007 \n",
      "2007 \n",
      "2007 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2007 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2009 \n",
      "2008 \n",
      "2008 \n",
      "2008 \n",
      "2009 \n",
      "2009 \n",
      "2009 \n",
      "2009 \n",
      "2009 \n",
      "2009 \n",
      "2010 \n",
      "2009 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2010 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2011 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2012 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2014 \n",
      "2013 \n",
      "2013 \n",
      "2013 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2013 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2014 \n",
      "2015 \n",
      "2014 \n",
      "2014 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2014 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2015 \n",
      "2016 \n",
      "2016 \n",
      "2015 \n",
      "2016 \n",
      "2016 \n",
      "2015 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2017 \n",
      "2016 \n",
      "2017 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2017 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2016 \n",
      "2016 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2017 \n",
      "2016 \n",
      "2018 \n",
      "2018 \n",
      "2016 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2019 \n",
      "2019 \n",
      "2018 \n",
      "2018 \n",
      "2018 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2020 \n",
      "2020 \n",
      "2019 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2019 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2019 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n",
      "2020 \n"
     ]
    }
   ],
   "source": [
    "yy=driver.find_elements_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p\")\n",
    "for i in yy:\n",
    "    print(i.text)\n",
    "    Year.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Name\":Dataset_name[0:15],\n",
    "                \"Year\":Year[0:15],\n",
    "                \"Data type\":Data_type[0:15],\n",
    "                \"Attribute_type\":Attribute_type[0:15],\n",
    "                 \"Task\":Task[0:15],\n",
    "                 \"No of instances\":No_instances[0:15],\n",
    "                 \"No of atrributes\":No_attribute[0:15]\n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Data type</th>\n",
       "      <th>Attribute_type</th>\n",
       "      <th>Task</th>\n",
       "      <th>No of instances</th>\n",
       "      <th>No of atrributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Year</td>\n",
       "      <td>Classification (419)\\nRegression (129)\\nCluste...</td>\n",
       "      <td>Attribute Types</td>\n",
       "      <td>Default Task</td>\n",
       "      <td># Instances</td>\n",
       "      <td># Attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>1995</td>\n",
       "      <td>Categorical (38)\\nNumerical (376)\\nMixed (55)</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>1996</td>\n",
       "      <td>Multivariate (435)\\nUnivariate (27)\\nSequentia...</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>Classification</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td></td>\n",
       "      <td>Life Sciences (132)\\nPhysical Sciences (56)\\nC...</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td>1998</td>\n",
       "      <td>Less than 10 (142)\\n10 to 100 (253)\\nGreater t...</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Artificial Characters</td>\n",
       "      <td>1998</td>\n",
       "      <td>Less than 100 (32)\\n100 to 1000 (191)\\nGreater...</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>452</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Audiology (Original)</td>\n",
       "      <td>1992</td>\n",
       "      <td>Matrix (388)\\nNon-Matrix (171)</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Classification</td>\n",
       "      <td>6000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Audiology (Standardized)</td>\n",
       "      <td>1987</td>\n",
       "      <td>Table View  List View</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>226</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Auto MPG</td>\n",
       "      <td>1992</td>\n",
       "      <td>Abalone</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>226</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Automobile</td>\n",
       "      <td>1993</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Categorical, Real</td>\n",
       "      <td>Regression</td>\n",
       "      <td>398</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Badges</td>\n",
       "      <td>1987</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>Regression</td>\n",
       "      <td>205</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Balance Scale</td>\n",
       "      <td>1994</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td></td>\n",
       "      <td>Classification</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Balloons</td>\n",
       "      <td>1994</td>\n",
       "      <td>4177</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>625</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Breast Cancer</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Breast Cancer Wisconsin (Original)</td>\n",
       "      <td>1988</td>\n",
       "      <td>1995</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>Classification</td>\n",
       "      <td>286</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Name   Year  \\\n",
       "0                              Abalone   Year   \n",
       "1                                Adult  1995    \n",
       "2                            Annealing  1996    \n",
       "3         Anonymous Microsoft Web Data          \n",
       "4                           Arrhythmia  1998    \n",
       "5                Artificial Characters  1998    \n",
       "6                 Audiology (Original)  1992    \n",
       "7             Audiology (Standardized)  1987    \n",
       "8                             Auto MPG  1992    \n",
       "9                           Automobile  1993    \n",
       "10                              Badges  1987    \n",
       "11                       Balance Scale  1994    \n",
       "12                            Balloons  1994    \n",
       "13                       Breast Cancer          \n",
       "14  Breast Cancer Wisconsin (Original)  1988    \n",
       "\n",
       "                                            Data type  \\\n",
       "0   Classification (419)\\nRegression (129)\\nCluste...   \n",
       "1       Categorical (38)\\nNumerical (376)\\nMixed (55)   \n",
       "2   Multivariate (435)\\nUnivariate (27)\\nSequentia...   \n",
       "3   Life Sciences (132)\\nPhysical Sciences (56)\\nC...   \n",
       "4   Less than 10 (142)\\n10 to 100 (253)\\nGreater t...   \n",
       "5   Less than 100 (32)\\n100 to 1000 (191)\\nGreater...   \n",
       "6                      Matrix (388)\\nNon-Matrix (171)   \n",
       "7                               Table View  List View   \n",
       "8                                             Abalone   \n",
       "9                                       Multivariate    \n",
       "10                                    Classification    \n",
       "11                        Categorical, Integer, Real    \n",
       "12                                              4177    \n",
       "13                                                 8    \n",
       "14                                              1995    \n",
       "\n",
       "                 Attribute_type                  Task No of instances  \\\n",
       "0               Attribute Types          Default Task     # Instances   \n",
       "1   Categorical, Integer, Real        Classification            4177    \n",
       "2         Categorical, Integer        Classification           48842    \n",
       "3   Categorical, Integer, Real        Classification             798    \n",
       "4                  Categorical   Recommender-Systems           37711    \n",
       "5   Categorical, Integer, Real        Classification             452    \n",
       "6   Categorical, Integer, Real        Classification            6000    \n",
       "7                  Categorical        Classification             226    \n",
       "8                  Categorical        Classification             226    \n",
       "9            Categorical, Real            Regression             398    \n",
       "10  Categorical, Integer, Real            Regression             205    \n",
       "11                                    Classification             294    \n",
       "12                 Categorical        Classification             625    \n",
       "13                 Categorical        Classification              16    \n",
       "14                 Categorical        Classification             286    \n",
       "\n",
       "   No of atrributes  \n",
       "0      # Attributes  \n",
       "1                8   \n",
       "2               14   \n",
       "3               38   \n",
       "4              294   \n",
       "5              279   \n",
       "6                7   \n",
       "7                    \n",
       "8               69   \n",
       "9                8   \n",
       "10              26   \n",
       "11               1   \n",
       "12               4   \n",
       "13               4   \n",
       "14               9   "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billiboard.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\MARJAN\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.billboard.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/nav/ul/li[3]/a\")\n",
    "try:\n",
    "    hot.click()\n",
    "except ElementNotInteractableException:\n",
    "    driver.get(hot.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_name=[]\n",
    "Artist_name=[]\n",
    "Last_week_rank=[]\n",
    "Peak_rank=[]\n",
    "Weeks_on_board=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drivers License\n",
      "Mood\n",
      "Blinding Lights\n",
      "34+35\n",
      "Levitating\n",
      "Go Crazy\n",
      "Positions\n",
      "Save Your Tears\n",
      "Holy\n",
      "Whoopty\n",
      "Good Days\n",
      "Lonely\n",
      "What You Know Bout Love\n",
      "Therefore I Am\n",
      "For The Night\n",
      "Bang!\n",
      "Better Together\n",
      "Streets\n",
      "I Hope\n",
      "Body\n",
      "Dakiti\n",
      "Lemonade\n",
      "You Broke Me First.\n",
      "Laugh Now Cry Later\n",
      "My Ex's Best Friend\n",
      "Wasted On You\n",
      "You're Mines Still\n",
      "Anyone\n",
      "Willow\n",
      "On Me\n",
      "Good Time\n",
      "Sand In My Boots\n",
      "Throat Baby (Go Baby)\n",
      "Before You Go\n",
      "7 Summers\n",
      "Starting Over\n",
      "Back In Blood\n",
      "More Than My Hometown\n",
      "Finesse Out The Gang Way\n",
      "Hole In The Bottle\n",
      "Kings & Queens\n",
      "Best Friend\n",
      "Cry Baby\n",
      "Rockstar\n",
      "Afterglow\n",
      "Dynamite\n",
      "Happy Does\n",
      "Somebody's Problem\n",
      "Whats Poppin\n",
      "Put Your Records On\n",
      "Damage\n",
      "Without You\n",
      "Should've Ducked\n",
      "Just The Way\n",
      "Beers And Sunshine\n",
      "Down To One\n",
      "What's Your Country Song\n",
      "Beat Box\n",
      "Diamonds\n",
      "The Good Ones\n",
      "Golden\n",
      "Buss It\n",
      "Goosebumps\n",
      "Monsters\n",
      "Long Live\n",
      "Tyler Herro\n",
      "Cover Me Up\n",
      "Forever After All\n",
      "Back To The Streets\n",
      "Still Trappin'\n",
      "Gravity\n",
      "Bichota\n",
      "Bad Boy\n",
      "Baila Conmigo\n",
      "Lady\n",
      "Still Goin Down\n",
      "865\n",
      "So Done\n",
      "Warning\n",
      "Holiday\n",
      "Hell Of A View\n",
      "Girl Like Me\n",
      "Momma's House\n",
      "Big, Big Plans\n",
      "Monster\n",
      "Heat Waves\n",
      "Stay Down\n",
      "Somebody Like That\n",
      "Skin\n",
      "Moonwalking In Calabasas\n",
      "Kanye Krazy\n",
      "Pick Up Your Feelings\n",
      "You Got It\n",
      "One Too Many\n",
      "Backdoor\n",
      "Fake Woke\n",
      "Prisoner\n",
      "Dangerous\n",
      "Almost Maybes\n",
      "Mr. Right Now\n"
     ]
    }
   ],
   "source": [
    "sn=driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\")\n",
    "for i in sn:\n",
    "    print(i.text)\n",
    "    Song_name.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olivia Rodrigo\n",
      "24kGoldn Featuring iann dior\n",
      "The Weeknd\n",
      "Ariana Grande\n",
      "Dua Lipa Featuring DaBaby\n",
      "Chris Brown & Young Thug\n",
      "Ariana Grande\n",
      "The Weeknd\n",
      "Justin Bieber Featuring Chance The Rapper\n",
      "CJ\n",
      "SZA\n",
      "Justin Bieber & benny blanco\n",
      "Pop Smoke\n",
      "Billie Eilish\n",
      "Pop Smoke Featuring Lil Baby & DaBaby\n",
      "AJR\n",
      "Luke Combs\n",
      "Doja Cat\n",
      "Gabby Barrett Featuring Charlie Puth\n",
      "Megan Thee Stallion\n",
      "Bad Bunny & Jhay Cortez\n",
      "Internet Money & Gunna Featuring Don Toliver & NAV\n",
      "Tate McRae\n",
      "Drake Featuring Lil Durk\n",
      "Machine Gun Kelly X blackbear\n",
      "Morgan Wallen\n",
      "Yung Bleu Featuring Drake\n",
      "Justin Bieber\n",
      "Taylor Swift\n",
      "Lil Baby\n",
      "Niko Moon\n",
      "Morgan Wallen\n",
      "BRS Kash\n",
      "Lewis Capaldi\n",
      "Morgan Wallen\n",
      "Chris Stapleton\n",
      "Pooh Shiesty Featuring Lil Durk\n",
      "Morgan Wallen\n",
      "Lil Durk Featuring Lil Baby\n",
      "Kelsea Ballerini\n",
      "Ava Max\n",
      "Saweetie Featuring Doja Cat\n",
      "Megan Thee Stallion Featuring DaBaby\n",
      "DaBaby Featuring Roddy Ricch\n",
      "Ed Sheeran\n",
      "BTS\n",
      "Kenny Chesney\n",
      "Morgan Wallen\n",
      "Jack Harlow Featuring DaBaby, Tory Lanez & Lil Wayne\n",
      "Ritt Momney\n",
      "H.E.R.\n",
      "The Kid LAROI\n",
      "Lil Durk Featuring Pooh Shiesty\n",
      "Parmalee x Blanco Brown\n",
      "Darius Rucker\n",
      "Luke Bryan\n",
      "Thomas Rhett\n",
      "SpotemGottem\n",
      "Sam Smith\n",
      "Gabby Barrett\n",
      "Harry Styles\n",
      "Erica Banks\n",
      "Travis Scott & HVME\n",
      "All Time Low Featuring Demi Lovato & blackbear\n",
      "Florida Georgia Line\n",
      "Jack Harlow\n",
      "Morgan Wallen\n",
      "Luke Combs\n",
      "Saweetie Featuring Jhene Aiko\n",
      "Lil Durk & King Von\n",
      "Brent Faiyaz & DJ Dahi Featuring Tyler, The Creator\n",
      "Karol G\n",
      "Juice WRLD & Young Thug\n",
      "Selena Gomez With Rauw Alejandro\n",
      "Brett Young\n",
      "Morgan Wallen\n",
      "Morgan Wallen\n",
      "The Kid LAROI\n",
      "Morgan Wallen\n",
      "Lil Nas X\n",
      "Eric Church\n",
      "Black Eyed Peas X Shakira\n",
      "Dustin Lynch\n",
      "Chris Lane\n",
      "Shawn Mendes & Justin Bieber\n",
      "Glass Animals\n",
      "Lil Durk, 6LACK & Young Thug\n",
      "Tenille Arts\n",
      "Sabrina Carpenter\n",
      "DDG\n",
      "Lil Durk\n",
      "Jazmine Sullivan\n",
      "VEDO\n",
      "Keith Urban Duet With P!nk\n",
      "Lil Durk\n",
      "Tom MacDonald\n",
      "Miley Cyrus Featuring Dua Lipa\n",
      "Morgan Wallen\n",
      "Jordan Davis\n",
      "21 Savage & Metro Boomin Featuring Drake\n"
     ]
    }
   ],
   "source": [
    "an=driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\")\n",
    "for i in an:\n",
    "    print(i.text)\n",
    "    Artist_name.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "14\n",
      "8\n",
      "16\n",
      "9\n",
      "13\n",
      "19\n",
      "12\n",
      "11\n",
      "10\n",
      "15\n",
      "25\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "27\n",
      "22\n",
      "34\n",
      "31\n",
      "32\n",
      "26\n",
      "28\n",
      "29\n",
      "37\n",
      "38\n",
      "24\n",
      "33\n",
      "23\n",
      "36\n",
      "40\n",
      "30\n",
      "-\n",
      "42\n",
      "35\n",
      "45\n",
      "54\n",
      "41\n",
      "39\n",
      "46\n",
      "50\n",
      "43\n",
      "44\n",
      "55\n",
      "52\n",
      "47\n",
      "-\n",
      "56\n",
      "53\n",
      "58\n",
      "59\n",
      "60\n",
      "49\n",
      "64\n",
      "65\n",
      "67\n",
      "73\n",
      "69\n",
      "68\n",
      "63\n",
      "74\n",
      "66\n",
      "61\n",
      "84\n",
      "-\n",
      "75\n",
      "57\n",
      "-\n",
      "86\n",
      "72\n",
      "77\n",
      "79\n",
      "76\n",
      "71\n",
      "90\n",
      "85\n",
      "87\n",
      "82\n",
      "70\n",
      "91\n",
      "-\n",
      "89\n",
      "48\n",
      "88\n",
      "-\n",
      "-\n",
      "95\n",
      "96\n",
      "-\n",
      "-\n",
      "81\n",
      "-\n",
      "-\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "lr=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\")\n",
    "for i in lr:\n",
    "    print(i.text)\n",
    "    Last_week_rank.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "5\n",
      "1\n",
      "8\n",
      "3\n",
      "10\n",
      "9\n",
      "12\n",
      "13\n",
      "2\n",
      "6\n",
      "8\n",
      "15\n",
      "18\n",
      "3\n",
      "12\n",
      "5\n",
      "6\n",
      "23\n",
      "2\n",
      "25\n",
      "9\n",
      "27\n",
      "6\n",
      "1\n",
      "29\n",
      "31\n",
      "32\n",
      "24\n",
      "9\n",
      "6\n",
      "25\n",
      "37\n",
      "15\n",
      "39\n",
      "40\n",
      "13\n",
      "39\n",
      "43\n",
      "1\n",
      "29\n",
      "1\n",
      "47\n",
      "25\n",
      "2\n",
      "50\n",
      "51\n",
      "47\n",
      "53\n",
      "54\n",
      "53\n",
      "56\n",
      "57\n",
      "58\n",
      "39\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "34\n",
      "52\n",
      "2\n",
      "58\n",
      "53\n",
      "71\n",
      "72\n",
      "22\n",
      "74\n",
      "75\n",
      "45\n",
      "46\n",
      "59\n",
      "42\n",
      "37\n",
      "81\n",
      "82\n",
      "83\n",
      "42\n",
      "8\n",
      "86\n",
      "73\n",
      "88\n",
      "48\n",
      "82\n",
      "91\n",
      "92\n",
      "87\n",
      "62\n",
      "62\n",
      "96\n",
      "54\n",
      "62\n",
      "95\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "pk=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\")\n",
    "for i in pk:\n",
    "    print(i.text)\n",
    "    Peak_rank.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "26\n",
      "61\n",
      "14\n",
      "18\n",
      "39\n",
      "15\n",
      "8\n",
      "20\n",
      "13\n",
      "6\n",
      "16\n",
      "22\n",
      "13\n",
      "31\n",
      "31\n",
      "18\n",
      "4\n",
      "58\n",
      "11\n",
      "14\n",
      "25\n",
      "24\n",
      "25\n",
      "25\n",
      "4\n",
      "9\n",
      "5\n",
      "8\n",
      "9\n",
      "18\n",
      "4\n",
      "16\n",
      "50\n",
      "24\n",
      "23\n",
      "5\n",
      "32\n",
      "1\n",
      "16\n",
      "26\n",
      "4\n",
      "9\n",
      "41\n",
      "7\n",
      "24\n",
      "12\n",
      "8\n",
      "51\n",
      "16\n",
      "8\n",
      "9\n",
      "1\n",
      "6\n",
      "8\n",
      "6\n",
      "7\n",
      "3\n",
      "20\n",
      "6\n",
      "15\n",
      "4\n",
      "3\n",
      "6\n",
      "5\n",
      "15\n",
      "5\n",
      "15\n",
      "11\n",
      "6\n",
      "1\n",
      "10\n",
      "3\n",
      "1\n",
      "4\n",
      "7\n",
      "4\n",
      "13\n",
      "4\n",
      "12\n",
      "4\n",
      "4\n",
      "4\n",
      "18\n",
      "11\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "6\n",
      "1\n",
      "2\n",
      "6\n",
      "8\n",
      "4\n",
      "1\n",
      "11\n",
      "3\n",
      "2\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "wb=driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\")\n",
    "for i in wb:\n",
    "    print(i.text)\n",
    "    Weeks_on_board.append(i.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"Song_name\":Song_name[0:15],\n",
    "                \"Artist_name\":Artist_name[0:15],\n",
    "                \"Last_week_rank\":Last_week_rank[0:15],\n",
    "                \"Peak_rank\":Peak_rank[0:15],\n",
    "                 \"Weeks_on_board\":Weeks_on_board[0:15]\n",
    "                 \n",
    "                  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_name</th>\n",
       "      <th>Artist_name</th>\n",
       "      <th>Last_week_rank</th>\n",
       "      <th>Peak_rank</th>\n",
       "      <th>Weeks_on_board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drivers License</td>\n",
       "      <td>Olivia Rodrigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mood</td>\n",
       "      <td>24kGoldn Featuring iann dior</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blinding Lights</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34+35</td>\n",
       "      <td>Ariana Grande</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Levitating</td>\n",
       "      <td>Dua Lipa Featuring DaBaby</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Go Crazy</td>\n",
       "      <td>Chris Brown &amp; Young Thug</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Positions</td>\n",
       "      <td>Ariana Grande</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Save Your Tears</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Holy</td>\n",
       "      <td>Justin Bieber Featuring Chance The Rapper</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Whoopty</td>\n",
       "      <td>CJ</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Good Days</td>\n",
       "      <td>SZA</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lonely</td>\n",
       "      <td>Justin Bieber &amp; benny blanco</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What You Know Bout Love</td>\n",
       "      <td>Pop Smoke</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Therefore I Am</td>\n",
       "      <td>Billie Eilish</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>For The Night</td>\n",
       "      <td>Pop Smoke Featuring Lil Baby &amp; DaBaby</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Song_name                                Artist_name  \\\n",
       "0           Drivers License                             Olivia Rodrigo   \n",
       "1                      Mood               24kGoldn Featuring iann dior   \n",
       "2           Blinding Lights                                 The Weeknd   \n",
       "3                     34+35                              Ariana Grande   \n",
       "4                Levitating                  Dua Lipa Featuring DaBaby   \n",
       "5                  Go Crazy                   Chris Brown & Young Thug   \n",
       "6                 Positions                              Ariana Grande   \n",
       "7           Save Your Tears                                 The Weeknd   \n",
       "8                      Holy  Justin Bieber Featuring Chance The Rapper   \n",
       "9                   Whoopty                                         CJ   \n",
       "10                Good Days                                        SZA   \n",
       "11                   Lonely               Justin Bieber & benny blanco   \n",
       "12  What You Know Bout Love                                  Pop Smoke   \n",
       "13           Therefore I Am                              Billie Eilish   \n",
       "14            For The Night      Pop Smoke Featuring Lil Baby & DaBaby   \n",
       "\n",
       "   Last_week_rank Peak_rank Weeks_on_board  \n",
       "0               1         1              4  \n",
       "1               2         1             26  \n",
       "2               3         1             61  \n",
       "3               4         2             14  \n",
       "4               5         5             18  \n",
       "5               6         5             39  \n",
       "6               7         1             15  \n",
       "7              14         8              8  \n",
       "8               8         3             20  \n",
       "9              16        10             13  \n",
       "10              9         9              6  \n",
       "11             13        12             16  \n",
       "12             19        13             22  \n",
       "13             12         2             13  \n",
       "14             11         6             31  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
